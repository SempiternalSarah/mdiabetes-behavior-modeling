{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23791e19-fa29-4321-9a92-62c66249e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "from utils.behavior_data import BehaviorData\n",
    "from visuals import Plotter\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.state_data import StateData\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ccab30-3fb9-4990-8492-e7d81a539a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.1883 train acc: 54.581% test acc: 56.349% train exerAcc: 45.217% test exerAcc: 38.095%\n",
      "5\t train loss: 0.1676 train acc: 57.331% test acc: 59.768% train exerAcc: 47.391% test exerAcc: 52.381%\n",
      "10\t train loss: 0.1606 train acc: 57.111% test acc: 60.745% train exerAcc: 48.696% test exerAcc: 52.381%\n",
      "15\t train loss: 0.1567 train acc: 57.646% test acc: 60.806% train exerAcc: 50.000% test exerAcc: 46.032%\n",
      "20\t train loss: 0.1547 train acc: 57.787% test acc: 60.806% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "25\t train loss: 0.1522 train acc: 58.117% test acc: 61.233% train exerAcc: 50.870% test exerAcc: 46.032%\n",
      "30\t train loss: 0.1507 train acc: 58.683% test acc: 61.294% train exerAcc: 50.000% test exerAcc: 46.032%\n",
      "35\t train loss: 0.1489 train acc: 58.542% test acc: 61.783% train exerAcc: 50.435% test exerAcc: 46.032%\n",
      "40\t train loss: 0.1488 train acc: 59.060% test acc: 61.538% train exerAcc: 53.478% test exerAcc: 46.032%\n",
      "45\t train loss: 0.1481 train acc: 59.092% test acc: 60.623% train exerAcc: 53.043% test exerAcc: 44.444%\n",
      "50\t train loss: 0.1471 train acc: 59.186% test acc: 61.111% train exerAcc: 54.348% test exerAcc: 49.206%\n",
      "55\t train loss: 0.1460 train acc: 59.453% test acc: 61.294% train exerAcc: 56.087% test exerAcc: 44.444%\n",
      "60\t train loss: 0.1461 train acc: 59.485% test acc: 61.783% train exerAcc: 55.217% test exerAcc: 47.619%\n",
      "65\t train loss: 0.1449 train acc: 59.642% test acc: 61.844% train exerAcc: 56.087% test exerAcc: 46.032%\n",
      "70\t train loss: 0.1446 train acc: 59.909% test acc: 61.905% train exerAcc: 56.522% test exerAcc: 49.206%\n",
      "75\t train loss: 0.1445 train acc: 60.255% test acc: 61.966% train exerAcc: 57.391% test exerAcc: 52.381%\n",
      "80\t train loss: 0.1437 train acc: 60.396% test acc: 61.600% train exerAcc: 56.522% test exerAcc: 49.206%\n",
      "85\t train loss: 0.1431 train acc: 60.710% test acc: 61.722% train exerAcc: 57.391% test exerAcc: 49.206%\n",
      "90\t train loss: 0.1425 train acc: 60.773% test acc: 61.966% train exerAcc: 56.957% test exerAcc: 49.206%\n",
      "95\t train loss: 0.1422 train acc: 61.009% test acc: 61.783% train exerAcc: 55.217% test exerAcc: 47.619%\n",
      "100\t train loss: 0.1419 train acc: 61.245% test acc: 62.088% train exerAcc: 57.826% test exerAcc: 49.206%\n",
      "105\t train loss: 0.1411 train acc: 61.528% test acc: 61.783% train exerAcc: 60.000% test exerAcc: 49.206%\n",
      "110\t train loss: 0.1407 train acc: 61.653% test acc: 62.149% train exerAcc: 58.261% test exerAcc: 52.381%\n",
      "115\t train loss: 0.1404 train acc: 61.685% test acc: 62.027% train exerAcc: 57.826% test exerAcc: 52.381%\n",
      "120\t train loss: 0.1405 train acc: 61.669% test acc: 61.905% train exerAcc: 58.261% test exerAcc: 53.968%\n",
      "125\t train loss: 0.1402 train acc: 61.732% test acc: 61.722% train exerAcc: 57.826% test exerAcc: 53.968%\n",
      "130\t train loss: 0.1399 train acc: 62.015% test acc: 62.088% train exerAcc: 59.130% test exerAcc: 55.556%\n",
      "135\t train loss: 0.1393 train acc: 62.093% test acc: 61.600% train exerAcc: 60.000% test exerAcc: 52.381%\n",
      "140\t train loss: 0.1390 train acc: 62.093% test acc: 61.783% train exerAcc: 59.130% test exerAcc: 50.794%\n",
      "145\t train loss: 0.1396 train acc: 62.423% test acc: 61.966% train exerAcc: 60.435% test exerAcc: 55.556%\n",
      "150\t train loss: 0.1383 train acc: 62.361% test acc: 62.271% train exerAcc: 60.435% test exerAcc: 52.381%\n",
      "155\t train loss: 0.1391 train acc: 62.565% test acc: 62.149% train exerAcc: 60.870% test exerAcc: 58.730%\n",
      "160\t train loss: 0.1384 train acc: 62.471% test acc: 62.088% train exerAcc: 62.174% test exerAcc: 55.556%\n",
      "165\t train loss: 0.1383 train acc: 62.565% test acc: 62.271% train exerAcc: 62.174% test exerAcc: 52.381%\n",
      "170\t train loss: 0.1378 train acc: 62.706% test acc: 61.661% train exerAcc: 62.174% test exerAcc: 49.206%\n",
      "175\t train loss: 0.1384 train acc: 62.659% test acc: 62.210% train exerAcc: 62.609% test exerAcc: 55.556%\n",
      "180\t train loss: 0.1374 train acc: 62.659% test acc: 62.454% train exerAcc: 63.043% test exerAcc: 53.968%\n",
      "185\t train loss: 0.1382 train acc: 62.706% test acc: 62.210% train exerAcc: 62.609% test exerAcc: 55.556%\n",
      "190\t train loss: 0.1371 train acc: 62.832% test acc: 61.844% train exerAcc: 62.609% test exerAcc: 52.381%\n",
      "195\t train loss: 0.1368 train acc: 62.832% test acc: 62.454% train exerAcc: 63.478% test exerAcc: 55.556%\n",
      "200\t train loss: 0.1374 train acc: 62.769% test acc: 61.783% train exerAcc: 61.739% test exerAcc: 52.381%\n",
      "205\t train loss: 0.1375 train acc: 62.801% test acc: 62.027% train exerAcc: 63.913% test exerAcc: 53.968%\n",
      "210\t train loss: 0.1367 train acc: 62.926% test acc: 61.477% train exerAcc: 64.348% test exerAcc: 50.794%\n",
      "215\t train loss: 0.1369 train acc: 62.706% test acc: 62.027% train exerAcc: 63.478% test exerAcc: 49.206%\n",
      "220\t train loss: 0.1365 train acc: 62.895% test acc: 62.210% train exerAcc: 63.478% test exerAcc: 50.794%\n",
      "225\t train loss: 0.1365 train acc: 63.021% test acc: 60.989% train exerAcc: 63.913% test exerAcc: 53.968%\n",
      "230\t train loss: 0.1365 train acc: 63.068% test acc: 61.477% train exerAcc: 64.348% test exerAcc: 52.381%\n",
      "235\t train loss: 0.1363 train acc: 63.021% test acc: 61.294% train exerAcc: 64.348% test exerAcc: 52.381%\n",
      "240\t train loss: 0.1357 train acc: 63.068% test acc: 61.661% train exerAcc: 65.217% test exerAcc: 52.381%\n",
      "245\t train loss: 0.1356 train acc: 63.083% test acc: 61.172% train exerAcc: 64.348% test exerAcc: 50.794%\n",
      "250\t train loss: 0.1350 train acc: 62.958% test acc: 60.867% train exerAcc: 64.783% test exerAcc: 50.794%\n",
      "255\t train loss: 0.1353 train acc: 63.131% test acc: 61.172% train exerAcc: 65.652% test exerAcc: 46.032%\n",
      "260\t train loss: 0.1365 train acc: 63.162% test acc: 61.233% train exerAcc: 64.783% test exerAcc: 50.794%\n",
      "265\t train loss: 0.1349 train acc: 63.319% test acc: 61.111% train exerAcc: 66.087% test exerAcc: 53.968%\n",
      "270\t train loss: 0.1352 train acc: 63.099% test acc: 60.684% train exerAcc: 63.913% test exerAcc: 47.619%\n",
      "275\t train loss: 0.1351 train acc: 63.178% test acc: 61.233% train exerAcc: 65.217% test exerAcc: 52.381%\n",
      "280\t train loss: 0.1349 train acc: 63.303% test acc: 61.233% train exerAcc: 64.783% test exerAcc: 47.619%\n",
      "285\t train loss: 0.1352 train acc: 63.209% test acc: 61.172% train exerAcc: 65.652% test exerAcc: 46.032%\n",
      "290\t train loss: 0.1352 train acc: 63.429% test acc: 60.928% train exerAcc: 66.087% test exerAcc: 47.619%\n",
      "295\t train loss: 0.1350 train acc: 63.382% test acc: 60.806% train exerAcc: 65.217% test exerAcc: 42.857%\n",
      "299\t train loss: 0.1348 train acc: 63.744% test acc: 60.989% train exerAcc: 65.652% test exerAcc: 50.794%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.2018 train acc: 16.747% test acc: 15.318% train exerAcc: 26.432% test exerAcc: 19.697%\n",
      "5\t train loss: 0.1810 train acc: 53.212% test acc: 51.922% train exerAcc: 49.339% test exerAcc: 48.485%\n",
      "10\t train loss: 0.1679 train acc: 54.155% test acc: 52.840% train exerAcc: 49.339% test exerAcc: 46.970%\n",
      "15\t train loss: 0.1612 train acc: 58.421% test acc: 57.889% train exerAcc: 47.577% test exerAcc: 51.515%\n",
      "20\t train loss: 0.1575 train acc: 58.373% test acc: 57.602% train exerAcc: 49.780% test exerAcc: 50.000%\n",
      "25\t train loss: 0.1545 train acc: 58.245% test acc: 57.659% train exerAcc: 51.101% test exerAcc: 53.030%\n",
      "30\t train loss: 0.1518 train acc: 58.421% test acc: 57.717% train exerAcc: 52.423% test exerAcc: 56.061%\n",
      "35\t train loss: 0.1513 train acc: 58.933% test acc: 57.774% train exerAcc: 52.863% test exerAcc: 56.061%\n",
      "40\t train loss: 0.1491 train acc: 58.996% test acc: 58.290% train exerAcc: 50.661% test exerAcc: 51.515%\n",
      "45\t train loss: 0.1484 train acc: 59.236% test acc: 58.577% train exerAcc: 53.304% test exerAcc: 56.061%\n",
      "50\t train loss: 0.1477 train acc: 59.380% test acc: 58.061% train exerAcc: 55.066% test exerAcc: 53.030%\n",
      "55\t train loss: 0.1469 train acc: 59.572% test acc: 58.405% train exerAcc: 56.388% test exerAcc: 50.000%\n",
      "60\t train loss: 0.1464 train acc: 59.556% test acc: 58.520% train exerAcc: 57.709% test exerAcc: 50.000%\n",
      "65\t train loss: 0.1450 train acc: 59.764% test acc: 58.520% train exerAcc: 58.150% test exerAcc: 46.970%\n",
      "70\t train loss: 0.1446 train acc: 60.179% test acc: 59.151% train exerAcc: 58.150% test exerAcc: 46.970%\n",
      "75\t train loss: 0.1443 train acc: 60.307% test acc: 59.266% train exerAcc: 60.352% test exerAcc: 51.515%\n",
      "80\t train loss: 0.1441 train acc: 60.403% test acc: 59.151% train exerAcc: 59.031% test exerAcc: 51.515%\n",
      "85\t train loss: 0.1438 train acc: 60.914% test acc: 59.094% train exerAcc: 59.912% test exerAcc: 48.485%\n",
      "90\t train loss: 0.1429 train acc: 60.962% test acc: 59.208% train exerAcc: 60.793% test exerAcc: 48.485%\n",
      "95\t train loss: 0.1426 train acc: 61.218% test acc: 59.380% train exerAcc: 60.793% test exerAcc: 48.485%\n",
      "100\t train loss: 0.1425 train acc: 61.329% test acc: 59.380% train exerAcc: 60.352% test exerAcc: 46.970%\n",
      "105\t train loss: 0.1414 train acc: 61.457% test acc: 59.266% train exerAcc: 59.912% test exerAcc: 48.485%\n",
      "110\t train loss: 0.1415 train acc: 61.441% test acc: 59.266% train exerAcc: 61.233% test exerAcc: 48.485%\n",
      "115\t train loss: 0.1417 train acc: 61.569% test acc: 59.094% train exerAcc: 62.115% test exerAcc: 48.485%\n",
      "120\t train loss: 0.1407 train acc: 61.601% test acc: 59.036% train exerAcc: 61.674% test exerAcc: 50.000%\n",
      "125\t train loss: 0.1405 train acc: 61.681% test acc: 58.749% train exerAcc: 62.115% test exerAcc: 46.970%\n",
      "130\t train loss: 0.1404 train acc: 61.905% test acc: 58.749% train exerAcc: 62.115% test exerAcc: 46.970%\n",
      "135\t train loss: 0.1398 train acc: 61.889% test acc: 59.094% train exerAcc: 62.115% test exerAcc: 48.485%\n",
      "140\t train loss: 0.1399 train acc: 61.905% test acc: 59.151% train exerAcc: 61.233% test exerAcc: 48.485%\n",
      "145\t train loss: 0.1397 train acc: 62.017% test acc: 59.495% train exerAcc: 62.996% test exerAcc: 46.970%\n",
      "150\t train loss: 0.1398 train acc: 62.049% test acc: 59.266% train exerAcc: 62.555% test exerAcc: 48.485%\n",
      "155\t train loss: 0.1396 train acc: 62.097% test acc: 59.323% train exerAcc: 63.436% test exerAcc: 48.485%\n",
      "160\t train loss: 0.1393 train acc: 62.224% test acc: 59.725% train exerAcc: 62.555% test exerAcc: 46.970%\n",
      "165\t train loss: 0.1389 train acc: 62.496% test acc: 59.667% train exerAcc: 61.674% test exerAcc: 46.970%\n",
      "170\t train loss: 0.1391 train acc: 62.560% test acc: 59.897% train exerAcc: 63.877% test exerAcc: 46.970%\n",
      "175\t train loss: 0.1386 train acc: 62.480% test acc: 59.897% train exerAcc: 62.996% test exerAcc: 45.455%\n",
      "180\t train loss: 0.1387 train acc: 62.592% test acc: 60.126% train exerAcc: 62.996% test exerAcc: 46.970%\n",
      "185\t train loss: 0.1390 train acc: 62.448% test acc: 60.126% train exerAcc: 63.436% test exerAcc: 46.970%\n",
      "190\t train loss: 0.1386 train acc: 62.784% test acc: 59.897% train exerAcc: 63.436% test exerAcc: 46.970%\n",
      "195\t train loss: 0.1391 train acc: 62.848% test acc: 60.069% train exerAcc: 62.996% test exerAcc: 48.485%\n",
      "200\t train loss: 0.1380 train acc: 62.640% test acc: 60.011% train exerAcc: 62.555% test exerAcc: 46.970%\n",
      "205\t train loss: 0.1378 train acc: 62.784% test acc: 60.184% train exerAcc: 63.436% test exerAcc: 46.970%\n",
      "210\t train loss: 0.1378 train acc: 62.720% test acc: 60.069% train exerAcc: 63.436% test exerAcc: 45.455%\n",
      "215\t train loss: 0.1378 train acc: 62.959% test acc: 59.954% train exerAcc: 63.436% test exerAcc: 46.970%\n",
      "220\t train loss: 0.1381 train acc: 62.848% test acc: 59.782% train exerAcc: 62.996% test exerAcc: 46.970%\n",
      "225\t train loss: 0.1375 train acc: 62.864% test acc: 59.897% train exerAcc: 63.436% test exerAcc: 45.455%\n",
      "230\t train loss: 0.1375 train acc: 63.039% test acc: 59.839% train exerAcc: 63.877% test exerAcc: 45.455%\n",
      "235\t train loss: 0.1373 train acc: 62.975% test acc: 59.839% train exerAcc: 62.996% test exerAcc: 46.970%\n",
      "240\t train loss: 0.1370 train acc: 63.023% test acc: 59.782% train exerAcc: 62.996% test exerAcc: 46.970%\n",
      "245\t train loss: 0.1370 train acc: 63.023% test acc: 59.839% train exerAcc: 62.115% test exerAcc: 45.455%\n",
      "250\t train loss: 0.1369 train acc: 63.183% test acc: 59.782% train exerAcc: 63.436% test exerAcc: 43.939%\n",
      "255\t train loss: 0.1372 train acc: 63.167% test acc: 59.323% train exerAcc: 62.996% test exerAcc: 45.455%\n",
      "260\t train loss: 0.1370 train acc: 63.247% test acc: 59.438% train exerAcc: 62.115% test exerAcc: 45.455%\n",
      "265\t train loss: 0.1366 train acc: 63.311% test acc: 59.151% train exerAcc: 63.436% test exerAcc: 45.455%\n",
      "270\t train loss: 0.1366 train acc: 63.359% test acc: 59.380% train exerAcc: 64.317% test exerAcc: 46.970%\n",
      "275\t train loss: 0.1366 train acc: 63.455% test acc: 59.839% train exerAcc: 63.877% test exerAcc: 45.455%\n",
      "280\t train loss: 0.1366 train acc: 63.231% test acc: 59.725% train exerAcc: 62.996% test exerAcc: 46.970%\n",
      "285\t train loss: 0.1360 train acc: 63.343% test acc: 59.552% train exerAcc: 63.436% test exerAcc: 46.970%\n",
      "290\t train loss: 0.1362 train acc: 63.439% test acc: 59.610% train exerAcc: 63.436% test exerAcc: 45.455%\n",
      "295\t train loss: 0.1358 train acc: 63.615% test acc: 59.495% train exerAcc: 64.317% test exerAcc: 45.455%\n",
      "299\t train loss: 0.1358 train acc: 63.263% test acc: 59.725% train exerAcc: 63.877% test exerAcc: 46.970%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.1961 train acc: 45.681% test acc: 42.297% train exerAcc: 48.133% test exerAcc: 50.000%\n",
      "5\t train loss: 0.1671 train acc: 58.183% test acc: 55.511% train exerAcc: 48.133% test exerAcc: 50.000%\n",
      "10\t train loss: 0.1607 train acc: 57.691% test acc: 54.847% train exerAcc: 48.963% test exerAcc: 50.000%\n",
      "15\t train loss: 0.1577 train acc: 58.553% test acc: 55.511% train exerAcc: 49.378% test exerAcc: 53.846%\n",
      "20\t train loss: 0.1544 train acc: 59.076% test acc: 55.644% train exerAcc: 52.282% test exerAcc: 48.077%\n",
      "25\t train loss: 0.1508 train acc: 58.830% test acc: 55.644% train exerAcc: 50.622% test exerAcc: 50.000%\n",
      "30\t train loss: 0.1483 train acc: 59.153% test acc: 56.308% train exerAcc: 54.772% test exerAcc: 55.769%\n",
      "35\t train loss: 0.1473 train acc: 59.615% test acc: 56.175% train exerAcc: 55.602% test exerAcc: 51.923%\n",
      "40\t train loss: 0.1464 train acc: 59.477% test acc: 55.710% train exerAcc: 55.602% test exerAcc: 51.923%\n",
      "45\t train loss: 0.1453 train acc: 60.046% test acc: 55.710% train exerAcc: 54.772% test exerAcc: 46.154%\n",
      "50\t train loss: 0.1450 train acc: 60.339% test acc: 56.242% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "55\t train loss: 0.1443 train acc: 60.462% test acc: 56.042% train exerAcc: 56.432% test exerAcc: 51.923%\n",
      "60\t train loss: 0.1437 train acc: 60.446% test acc: 56.441% train exerAcc: 55.602% test exerAcc: 53.846%\n",
      "65\t train loss: 0.1425 train acc: 60.631% test acc: 56.707% train exerAcc: 57.261% test exerAcc: 55.769%\n",
      "70\t train loss: 0.1427 train acc: 60.939% test acc: 56.640% train exerAcc: 57.676% test exerAcc: 53.846%\n",
      "75\t train loss: 0.1421 train acc: 61.093% test acc: 57.039% train exerAcc: 57.676% test exerAcc: 53.846%\n",
      "80\t train loss: 0.1414 train acc: 61.139% test acc: 56.773% train exerAcc: 58.091% test exerAcc: 48.077%\n",
      "85\t train loss: 0.1410 train acc: 61.293% test acc: 57.171% train exerAcc: 57.676% test exerAcc: 51.923%\n",
      "90\t train loss: 0.1403 train acc: 61.416% test acc: 57.105% train exerAcc: 58.091% test exerAcc: 53.846%\n",
      "95\t train loss: 0.1403 train acc: 61.601% test acc: 57.039% train exerAcc: 57.676% test exerAcc: 53.846%\n",
      "100\t train loss: 0.1396 train acc: 61.678% test acc: 57.039% train exerAcc: 59.336% test exerAcc: 51.923%\n",
      "105\t train loss: 0.1390 train acc: 61.740% test acc: 57.304% train exerAcc: 59.336% test exerAcc: 57.692%\n",
      "110\t train loss: 0.1389 train acc: 61.925% test acc: 56.574% train exerAcc: 59.751% test exerAcc: 59.615%\n",
      "115\t train loss: 0.1387 train acc: 61.940% test acc: 56.441% train exerAcc: 62.241% test exerAcc: 55.769%\n",
      "120\t train loss: 0.1384 train acc: 62.017% test acc: 56.308% train exerAcc: 59.336% test exerAcc: 53.846%\n",
      "125\t train loss: 0.1384 train acc: 62.294% test acc: 56.308% train exerAcc: 60.996% test exerAcc: 53.846%\n",
      "130\t train loss: 0.1377 train acc: 62.371% test acc: 56.441% train exerAcc: 60.166% test exerAcc: 50.000%\n",
      "135\t train loss: 0.1377 train acc: 62.648% test acc: 56.640% train exerAcc: 61.826% test exerAcc: 53.846%\n",
      "140\t train loss: 0.1374 train acc: 62.725% test acc: 56.707% train exerAcc: 60.581% test exerAcc: 50.000%\n",
      "145\t train loss: 0.1372 train acc: 62.787% test acc: 56.507% train exerAcc: 61.411% test exerAcc: 51.923%\n",
      "150\t train loss: 0.1372 train acc: 62.848% test acc: 56.640% train exerAcc: 60.996% test exerAcc: 51.923%\n",
      "155\t train loss: 0.1370 train acc: 62.756% test acc: 56.375% train exerAcc: 60.581% test exerAcc: 50.000%\n",
      "160\t train loss: 0.1368 train acc: 62.787% test acc: 56.441% train exerAcc: 60.996% test exerAcc: 51.923%\n",
      "165\t train loss: 0.1366 train acc: 62.972% test acc: 56.707% train exerAcc: 61.411% test exerAcc: 50.000%\n",
      "170\t train loss: 0.1364 train acc: 62.833% test acc: 56.109% train exerAcc: 61.411% test exerAcc: 48.077%\n",
      "175\t train loss: 0.1366 train acc: 63.125% test acc: 56.441% train exerAcc: 62.241% test exerAcc: 51.923%\n",
      "180\t train loss: 0.1363 train acc: 63.326% test acc: 56.042% train exerAcc: 61.826% test exerAcc: 48.077%\n",
      "185\t train loss: 0.1369 train acc: 63.279% test acc: 56.175% train exerAcc: 61.411% test exerAcc: 50.000%\n",
      "190\t train loss: 0.1364 train acc: 63.295% test acc: 56.042% train exerAcc: 60.996% test exerAcc: 50.000%\n",
      "195\t train loss: 0.1356 train acc: 63.433% test acc: 56.042% train exerAcc: 61.826% test exerAcc: 48.077%\n",
      "200\t train loss: 0.1352 train acc: 63.403% test acc: 56.375% train exerAcc: 61.826% test exerAcc: 55.769%\n",
      "205\t train loss: 0.1349 train acc: 63.495% test acc: 56.242% train exerAcc: 61.411% test exerAcc: 48.077%\n",
      "210\t train loss: 0.1347 train acc: 63.449% test acc: 56.242% train exerAcc: 61.411% test exerAcc: 50.000%\n",
      "215\t train loss: 0.1351 train acc: 63.495% test acc: 55.910% train exerAcc: 61.411% test exerAcc: 44.231%\n",
      "220\t train loss: 0.1350 train acc: 63.664% test acc: 56.109% train exerAcc: 63.071% test exerAcc: 46.154%\n",
      "225\t train loss: 0.1353 train acc: 63.557% test acc: 55.843% train exerAcc: 62.241% test exerAcc: 48.077%\n",
      "230\t train loss: 0.1348 train acc: 63.495% test acc: 56.175% train exerAcc: 60.996% test exerAcc: 51.923%\n",
      "235\t train loss: 0.1352 train acc: 63.695% test acc: 56.042% train exerAcc: 62.241% test exerAcc: 51.923%\n",
      "240\t train loss: 0.1349 train acc: 63.634% test acc: 56.109% train exerAcc: 62.656% test exerAcc: 53.846%\n",
      "245\t train loss: 0.1343 train acc: 63.711% test acc: 56.109% train exerAcc: 63.900% test exerAcc: 53.846%\n",
      "250\t train loss: 0.1342 train acc: 63.649% test acc: 56.042% train exerAcc: 61.826% test exerAcc: 53.846%\n",
      "255\t train loss: 0.1344 train acc: 63.834% test acc: 56.109% train exerAcc: 64.730% test exerAcc: 51.923%\n",
      "260\t train loss: 0.1344 train acc: 63.757% test acc: 56.175% train exerAcc: 63.485% test exerAcc: 51.923%\n",
      "265\t train loss: 0.1346 train acc: 63.865% test acc: 56.042% train exerAcc: 63.071% test exerAcc: 50.000%\n",
      "270\t train loss: 0.1341 train acc: 63.834% test acc: 55.777% train exerAcc: 63.485% test exerAcc: 53.846%\n",
      "275\t train loss: 0.1345 train acc: 63.880% test acc: 56.109% train exerAcc: 65.145% test exerAcc: 55.769%\n",
      "280\t train loss: 0.1342 train acc: 63.911% test acc: 55.843% train exerAcc: 63.485% test exerAcc: 53.846%\n",
      "285\t train loss: 0.1338 train acc: 63.957% test acc: 55.644% train exerAcc: 63.071% test exerAcc: 53.846%\n",
      "290\t train loss: 0.1338 train acc: 63.926% test acc: 55.777% train exerAcc: 62.241% test exerAcc: 53.846%\n",
      "295\t train loss: 0.1331 train acc: 64.142% test acc: 55.445% train exerAcc: 64.315% test exerAcc: 53.846%\n",
      "299\t train loss: 0.1338 train acc: 64.095% test acc: 55.312% train exerAcc: 65.145% test exerAcc: 53.846%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.1878 train acc: 49.316% test acc: 51.343% train exerAcc: 48.696% test exerAcc: 50.794%\n",
      "5\t train loss: 0.1748 train acc: 55.336% test acc: 57.814% train exerAcc: 47.826% test exerAcc: 52.381%\n",
      "10\t train loss: 0.1689 train acc: 56.294% test acc: 58.669% train exerAcc: 47.826% test exerAcc: 52.381%\n",
      "15\t train loss: 0.1649 train acc: 56.907% test acc: 59.829% train exerAcc: 49.565% test exerAcc: 52.381%\n",
      "20\t train loss: 0.1616 train acc: 57.379% test acc: 60.745% train exerAcc: 50.000% test exerAcc: 52.381%\n",
      "25\t train loss: 0.1593 train acc: 57.599% test acc: 60.501% train exerAcc: 50.435% test exerAcc: 50.794%\n",
      "30\t train loss: 0.1578 train acc: 57.693% test acc: 60.256% train exerAcc: 48.696% test exerAcc: 50.794%\n",
      "35\t train loss: 0.1561 train acc: 57.819% test acc: 60.440% train exerAcc: 50.000% test exerAcc: 47.619%\n",
      "40\t train loss: 0.1550 train acc: 57.709% test acc: 60.379% train exerAcc: 50.870% test exerAcc: 47.619%\n",
      "45\t train loss: 0.1544 train acc: 57.803% test acc: 60.501% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "50\t train loss: 0.1540 train acc: 57.976% test acc: 60.745% train exerAcc: 50.870% test exerAcc: 46.032%\n",
      "55\t train loss: 0.1526 train acc: 58.007% test acc: 60.684% train exerAcc: 50.870% test exerAcc: 46.032%\n",
      "60\t train loss: 0.1521 train acc: 57.929% test acc: 60.684% train exerAcc: 50.870% test exerAcc: 47.619%\n",
      "65\t train loss: 0.1522 train acc: 58.070% test acc: 60.867% train exerAcc: 52.174% test exerAcc: 47.619%\n",
      "70\t train loss: 0.1524 train acc: 58.133% test acc: 60.989% train exerAcc: 52.174% test exerAcc: 47.619%\n",
      "75\t train loss: 0.1513 train acc: 58.384% test acc: 60.928% train exerAcc: 52.609% test exerAcc: 47.619%\n",
      "80\t train loss: 0.1513 train acc: 58.259% test acc: 61.233% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "85\t train loss: 0.1504 train acc: 58.149% test acc: 61.416% train exerAcc: 50.870% test exerAcc: 47.619%\n",
      "90\t train loss: 0.1507 train acc: 58.322% test acc: 61.355% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "95\t train loss: 0.1499 train acc: 58.463% test acc: 61.416% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "100\t train loss: 0.1501 train acc: 58.557% test acc: 61.722% train exerAcc: 52.174% test exerAcc: 49.206%\n",
      "105\t train loss: 0.1496 train acc: 58.762% test acc: 61.600% train exerAcc: 50.435% test exerAcc: 46.032%\n",
      "110\t train loss: 0.1495 train acc: 58.777% test acc: 61.538% train exerAcc: 50.870% test exerAcc: 46.032%\n",
      "115\t train loss: 0.1491 train acc: 59.013% test acc: 62.027% train exerAcc: 51.739% test exerAcc: 46.032%\n",
      "120\t train loss: 0.1485 train acc: 58.950% test acc: 62.149% train exerAcc: 51.739% test exerAcc: 47.619%\n",
      "125\t train loss: 0.1485 train acc: 59.013% test acc: 61.600% train exerAcc: 51.739% test exerAcc: 47.619%\n",
      "130\t train loss: 0.1490 train acc: 59.076% test acc: 61.844% train exerAcc: 52.609% test exerAcc: 46.032%\n",
      "135\t train loss: 0.1485 train acc: 59.029% test acc: 61.844% train exerAcc: 51.304% test exerAcc: 46.032%\n",
      "140\t train loss: 0.1482 train acc: 59.060% test acc: 61.966% train exerAcc: 51.304% test exerAcc: 46.032%\n",
      "145\t train loss: 0.1484 train acc: 59.202% test acc: 62.149% train exerAcc: 51.304% test exerAcc: 46.032%\n",
      "150\t train loss: 0.1483 train acc: 59.186% test acc: 62.027% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "155\t train loss: 0.1472 train acc: 59.170% test acc: 61.844% train exerAcc: 52.174% test exerAcc: 44.444%\n",
      "160\t train loss: 0.1483 train acc: 58.982% test acc: 62.393% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "165\t train loss: 0.1473 train acc: 59.107% test acc: 62.149% train exerAcc: 51.739% test exerAcc: 49.206%\n",
      "170\t train loss: 0.1475 train acc: 59.154% test acc: 62.088% train exerAcc: 50.870% test exerAcc: 47.619%\n",
      "175\t train loss: 0.1477 train acc: 59.154% test acc: 62.088% train exerAcc: 51.304% test exerAcc: 47.619%\n",
      "180\t train loss: 0.1471 train acc: 59.123% test acc: 61.844% train exerAcc: 50.435% test exerAcc: 47.619%\n",
      "185\t train loss: 0.1478 train acc: 59.249% test acc: 62.088% train exerAcc: 51.304% test exerAcc: 49.206%\n",
      "190\t train loss: 0.1473 train acc: 59.217% test acc: 62.027% train exerAcc: 50.870% test exerAcc: 49.206%\n",
      "195\t train loss: 0.1474 train acc: 59.249% test acc: 61.966% train exerAcc: 52.609% test exerAcc: 49.206%\n",
      "200\t train loss: 0.1476 train acc: 59.343% test acc: 62.149% train exerAcc: 52.174% test exerAcc: 50.794%\n",
      "205\t train loss: 0.1468 train acc: 59.233% test acc: 61.966% train exerAcc: 51.304% test exerAcc: 49.206%\n",
      "210\t train loss: 0.1476 train acc: 59.280% test acc: 62.149% train exerAcc: 53.043% test exerAcc: 49.206%\n",
      "215\t train loss: 0.1470 train acc: 59.296% test acc: 62.271% train exerAcc: 52.174% test exerAcc: 52.381%\n",
      "220\t train loss: 0.1468 train acc: 59.233% test acc: 62.088% train exerAcc: 52.609% test exerAcc: 50.794%\n",
      "225\t train loss: 0.1470 train acc: 59.044% test acc: 61.966% train exerAcc: 52.609% test exerAcc: 49.206%\n",
      "230\t train loss: 0.1468 train acc: 59.312% test acc: 62.027% train exerAcc: 52.609% test exerAcc: 50.794%\n",
      "235\t train loss: 0.1470 train acc: 59.170% test acc: 61.783% train exerAcc: 52.174% test exerAcc: 49.206%\n",
      "240\t train loss: 0.1468 train acc: 59.264% test acc: 62.088% train exerAcc: 52.174% test exerAcc: 50.794%\n",
      "245\t train loss: 0.1466 train acc: 59.327% test acc: 61.783% train exerAcc: 53.043% test exerAcc: 49.206%\n",
      "250\t train loss: 0.1467 train acc: 59.202% test acc: 61.783% train exerAcc: 52.174% test exerAcc: 49.206%\n",
      "255\t train loss: 0.1466 train acc: 59.296% test acc: 61.905% train exerAcc: 52.174% test exerAcc: 50.794%\n",
      "260\t train loss: 0.1466 train acc: 59.327% test acc: 62.027% train exerAcc: 52.174% test exerAcc: 50.794%\n",
      "265\t train loss: 0.1469 train acc: 59.249% test acc: 61.722% train exerAcc: 52.174% test exerAcc: 50.794%\n",
      "270\t train loss: 0.1463 train acc: 59.343% test acc: 61.722% train exerAcc: 52.609% test exerAcc: 49.206%\n",
      "275\t train loss: 0.1468 train acc: 59.249% test acc: 61.844% train exerAcc: 52.174% test exerAcc: 50.794%\n",
      "280\t train loss: 0.1466 train acc: 59.296% test acc: 62.027% train exerAcc: 53.478% test exerAcc: 50.794%\n",
      "285\t train loss: 0.1467 train acc: 59.296% test acc: 61.905% train exerAcc: 51.739% test exerAcc: 50.794%\n",
      "290\t train loss: 0.1467 train acc: 59.264% test acc: 61.966% train exerAcc: 52.609% test exerAcc: 52.381%\n",
      "295\t train loss: 0.1467 train acc: 59.202% test acc: 61.905% train exerAcc: 51.304% test exerAcc: 53.968%\n",
      "300\t train loss: 0.1466 train acc: 59.249% test acc: 61.783% train exerAcc: 53.043% test exerAcc: 52.381%\n",
      "305\t train loss: 0.1462 train acc: 59.264% test acc: 61.905% train exerAcc: 52.609% test exerAcc: 52.381%\n",
      "310\t train loss: 0.1462 train acc: 59.249% test acc: 62.027% train exerAcc: 52.609% test exerAcc: 52.381%\n",
      "315\t train loss: 0.1462 train acc: 59.264% test acc: 61.844% train exerAcc: 52.609% test exerAcc: 52.381%\n",
      "320\t train loss: 0.1463 train acc: 59.264% test acc: 61.722% train exerAcc: 52.609% test exerAcc: 50.794%\n",
      "325\t train loss: 0.1463 train acc: 59.186% test acc: 61.600% train exerAcc: 52.609% test exerAcc: 53.968%\n",
      "330\t train loss: 0.1464 train acc: 59.343% test acc: 61.966% train exerAcc: 52.609% test exerAcc: 50.794%\n",
      "335\t train loss: 0.1462 train acc: 59.375% test acc: 61.538% train exerAcc: 53.478% test exerAcc: 49.206%\n",
      "340\t train loss: 0.1456 train acc: 59.453% test acc: 61.905% train exerAcc: 53.043% test exerAcc: 53.968%\n",
      "345\t train loss: 0.1464 train acc: 59.186% test acc: 61.905% train exerAcc: 52.609% test exerAcc: 53.968%\n",
      "350\t train loss: 0.1461 train acc: 59.296% test acc: 61.477% train exerAcc: 53.478% test exerAcc: 49.206%\n",
      "355\t train loss: 0.1459 train acc: 59.280% test acc: 61.844% train exerAcc: 53.043% test exerAcc: 53.968%\n",
      "360\t train loss: 0.1460 train acc: 59.296% test acc: 61.600% train exerAcc: 53.478% test exerAcc: 52.381%\n",
      "365\t train loss: 0.1461 train acc: 59.202% test acc: 61.966% train exerAcc: 53.043% test exerAcc: 52.381%\n",
      "370\t train loss: 0.1454 train acc: 59.154% test acc: 61.783% train exerAcc: 52.609% test exerAcc: 53.968%\n",
      "375\t train loss: 0.1462 train acc: 59.359% test acc: 61.538% train exerAcc: 53.478% test exerAcc: 52.381%\n",
      "380\t train loss: 0.1456 train acc: 59.233% test acc: 61.477% train exerAcc: 53.043% test exerAcc: 52.381%\n",
      "385\t train loss: 0.1456 train acc: 59.327% test acc: 61.600% train exerAcc: 53.043% test exerAcc: 53.968%\n",
      "390\t train loss: 0.1462 train acc: 59.107% test acc: 61.538% train exerAcc: 52.174% test exerAcc: 52.381%\n",
      "395\t train loss: 0.1462 train acc: 59.233% test acc: 61.538% train exerAcc: 53.043% test exerAcc: 53.968%\n",
      "399\t train loss: 0.1454 train acc: 59.280% test acc: 62.027% train exerAcc: 53.043% test exerAcc: 53.968%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.1931 train acc: 36.258% test acc: 38.382% train exerAcc: 29.515% test exerAcc: 31.818%\n",
      "5\t train loss: 0.1792 train acc: 52.956% test acc: 53.471% train exerAcc: 47.137% test exerAcc: 48.485%\n",
      "10\t train loss: 0.1701 train acc: 56.568% test acc: 56.799% train exerAcc: 48.899% test exerAcc: 48.485%\n",
      "15\t train loss: 0.1656 train acc: 57.367% test acc: 57.143% train exerAcc: 49.339% test exerAcc: 48.485%\n",
      "20\t train loss: 0.1616 train acc: 58.086% test acc: 57.258% train exerAcc: 49.339% test exerAcc: 48.485%\n",
      "25\t train loss: 0.1590 train acc: 58.070% test acc: 58.003% train exerAcc: 49.339% test exerAcc: 48.485%\n",
      "30\t train loss: 0.1578 train acc: 58.213% test acc: 58.118% train exerAcc: 49.339% test exerAcc: 48.485%\n",
      "35\t train loss: 0.1571 train acc: 58.453% test acc: 58.233% train exerAcc: 50.220% test exerAcc: 48.485%\n",
      "40\t train loss: 0.1552 train acc: 58.533% test acc: 58.348% train exerAcc: 50.220% test exerAcc: 50.000%\n",
      "45\t train loss: 0.1544 train acc: 58.597% test acc: 58.577% train exerAcc: 48.899% test exerAcc: 50.000%\n",
      "50\t train loss: 0.1536 train acc: 58.709% test acc: 58.061% train exerAcc: 49.780% test exerAcc: 48.485%\n",
      "55\t train loss: 0.1525 train acc: 58.917% test acc: 58.520% train exerAcc: 51.101% test exerAcc: 50.000%\n",
      "60\t train loss: 0.1519 train acc: 59.076% test acc: 58.692% train exerAcc: 51.542% test exerAcc: 50.000%\n",
      "65\t train loss: 0.1515 train acc: 59.060% test acc: 58.692% train exerAcc: 50.661% test exerAcc: 51.515%\n",
      "70\t train loss: 0.1513 train acc: 58.965% test acc: 58.635% train exerAcc: 49.780% test exerAcc: 53.030%\n",
      "75\t train loss: 0.1500 train acc: 58.933% test acc: 58.520% train exerAcc: 48.899% test exerAcc: 48.485%\n",
      "80\t train loss: 0.1500 train acc: 58.981% test acc: 58.348% train exerAcc: 49.780% test exerAcc: 48.485%\n",
      "85\t train loss: 0.1496 train acc: 58.917% test acc: 58.290% train exerAcc: 49.780% test exerAcc: 48.485%\n",
      "90\t train loss: 0.1489 train acc: 59.204% test acc: 58.520% train exerAcc: 50.661% test exerAcc: 48.485%\n",
      "95\t train loss: 0.1492 train acc: 59.204% test acc: 58.749% train exerAcc: 51.101% test exerAcc: 48.485%\n",
      "100\t train loss: 0.1485 train acc: 59.236% test acc: 58.807% train exerAcc: 51.101% test exerAcc: 48.485%\n",
      "105\t train loss: 0.1480 train acc: 59.204% test acc: 58.864% train exerAcc: 51.542% test exerAcc: 48.485%\n",
      "110\t train loss: 0.1487 train acc: 59.220% test acc: 59.036% train exerAcc: 52.863% test exerAcc: 48.485%\n",
      "115\t train loss: 0.1480 train acc: 59.284% test acc: 58.921% train exerAcc: 51.982% test exerAcc: 48.485%\n",
      "120\t train loss: 0.1479 train acc: 59.284% test acc: 58.692% train exerAcc: 52.863% test exerAcc: 48.485%\n",
      "125\t train loss: 0.1476 train acc: 59.252% test acc: 58.979% train exerAcc: 51.542% test exerAcc: 50.000%\n",
      "130\t train loss: 0.1471 train acc: 59.444% test acc: 58.979% train exerAcc: 52.863% test exerAcc: 48.485%\n",
      "135\t train loss: 0.1475 train acc: 59.284% test acc: 58.979% train exerAcc: 51.542% test exerAcc: 48.485%\n",
      "140\t train loss: 0.1472 train acc: 59.460% test acc: 58.692% train exerAcc: 51.982% test exerAcc: 50.000%\n",
      "145\t train loss: 0.1470 train acc: 59.556% test acc: 58.979% train exerAcc: 51.982% test exerAcc: 50.000%\n",
      "150\t train loss: 0.1472 train acc: 59.492% test acc: 58.979% train exerAcc: 51.982% test exerAcc: 50.000%\n",
      "155\t train loss: 0.1462 train acc: 59.444% test acc: 58.807% train exerAcc: 51.982% test exerAcc: 50.000%\n",
      "160\t train loss: 0.1464 train acc: 59.588% test acc: 59.094% train exerAcc: 52.423% test exerAcc: 54.545%\n",
      "165\t train loss: 0.1467 train acc: 59.524% test acc: 58.692% train exerAcc: 51.982% test exerAcc: 50.000%\n",
      "170\t train loss: 0.1468 train acc: 59.636% test acc: 58.807% train exerAcc: 52.423% test exerAcc: 54.545%\n",
      "175\t train loss: 0.1464 train acc: 59.620% test acc: 58.749% train exerAcc: 51.542% test exerAcc: 54.545%\n",
      "180\t train loss: 0.1460 train acc: 59.636% test acc: 58.979% train exerAcc: 51.542% test exerAcc: 56.061%\n",
      "185\t train loss: 0.1460 train acc: 59.588% test acc: 58.807% train exerAcc: 51.542% test exerAcc: 54.545%\n",
      "190\t train loss: 0.1463 train acc: 59.827% test acc: 58.807% train exerAcc: 51.982% test exerAcc: 53.030%\n",
      "195\t train loss: 0.1459 train acc: 59.795% test acc: 58.692% train exerAcc: 51.982% test exerAcc: 53.030%\n",
      "200\t train loss: 0.1455 train acc: 59.636% test acc: 59.208% train exerAcc: 51.982% test exerAcc: 56.061%\n",
      "205\t train loss: 0.1459 train acc: 59.380% test acc: 59.266% train exerAcc: 51.101% test exerAcc: 53.030%\n",
      "210\t train loss: 0.1454 train acc: 59.700% test acc: 58.749% train exerAcc: 51.542% test exerAcc: 51.515%\n",
      "215\t train loss: 0.1456 train acc: 59.700% test acc: 59.208% train exerAcc: 52.423% test exerAcc: 57.576%\n",
      "220\t train loss: 0.1457 train acc: 59.524% test acc: 59.380% train exerAcc: 50.661% test exerAcc: 56.061%\n",
      "225\t train loss: 0.1455 train acc: 59.444% test acc: 59.036% train exerAcc: 51.101% test exerAcc: 56.061%\n",
      "230\t train loss: 0.1455 train acc: 59.556% test acc: 59.151% train exerAcc: 51.982% test exerAcc: 53.030%\n",
      "235\t train loss: 0.1454 train acc: 59.652% test acc: 59.036% train exerAcc: 51.542% test exerAcc: 54.545%\n",
      "240\t train loss: 0.1455 train acc: 59.652% test acc: 59.151% train exerAcc: 52.423% test exerAcc: 56.061%\n",
      "245\t train loss: 0.1452 train acc: 59.700% test acc: 59.094% train exerAcc: 53.304% test exerAcc: 53.030%\n",
      "250\t train loss: 0.1449 train acc: 59.668% test acc: 59.036% train exerAcc: 52.863% test exerAcc: 56.061%\n",
      "255\t train loss: 0.1446 train acc: 59.716% test acc: 58.979% train exerAcc: 53.304% test exerAcc: 56.061%\n",
      "260\t train loss: 0.1456 train acc: 59.716% test acc: 58.921% train exerAcc: 52.423% test exerAcc: 53.030%\n",
      "265\t train loss: 0.1452 train acc: 59.572% test acc: 59.266% train exerAcc: 52.863% test exerAcc: 54.545%\n",
      "270\t train loss: 0.1450 train acc: 59.652% test acc: 59.323% train exerAcc: 52.863% test exerAcc: 56.061%\n",
      "275\t train loss: 0.1452 train acc: 59.748% test acc: 59.151% train exerAcc: 51.982% test exerAcc: 54.545%\n",
      "280\t train loss: 0.1452 train acc: 59.700% test acc: 59.151% train exerAcc: 52.863% test exerAcc: 54.545%\n",
      "285\t train loss: 0.1453 train acc: 59.604% test acc: 58.979% train exerAcc: 52.423% test exerAcc: 56.061%\n",
      "290\t train loss: 0.1454 train acc: 59.556% test acc: 59.266% train exerAcc: 52.863% test exerAcc: 54.545%\n",
      "295\t train loss: 0.1453 train acc: 59.684% test acc: 59.036% train exerAcc: 51.982% test exerAcc: 54.545%\n",
      "300\t train loss: 0.1450 train acc: 59.875% test acc: 59.208% train exerAcc: 53.304% test exerAcc: 56.061%\n",
      "305\t train loss: 0.1454 train acc: 59.748% test acc: 58.979% train exerAcc: 53.304% test exerAcc: 56.061%\n",
      "310\t train loss: 0.1445 train acc: 59.843% test acc: 59.266% train exerAcc: 52.863% test exerAcc: 54.545%\n",
      "315\t train loss: 0.1448 train acc: 59.732% test acc: 58.921% train exerAcc: 53.304% test exerAcc: 56.061%\n",
      "320\t train loss: 0.1445 train acc: 59.636% test acc: 58.807% train exerAcc: 51.542% test exerAcc: 56.061%\n",
      "325\t train loss: 0.1452 train acc: 59.492% test acc: 58.979% train exerAcc: 51.982% test exerAcc: 56.061%\n",
      "330\t train loss: 0.1447 train acc: 59.939% test acc: 59.323% train exerAcc: 54.626% test exerAcc: 53.030%\n",
      "335\t train loss: 0.1446 train acc: 59.668% test acc: 58.692% train exerAcc: 52.423% test exerAcc: 53.030%\n",
      "340\t train loss: 0.1447 train acc: 59.684% test acc: 59.208% train exerAcc: 52.423% test exerAcc: 53.030%\n",
      "345\t train loss: 0.1448 train acc: 59.508% test acc: 59.151% train exerAcc: 52.863% test exerAcc: 56.061%\n",
      "350\t train loss: 0.1446 train acc: 59.652% test acc: 58.921% train exerAcc: 51.101% test exerAcc: 54.545%\n",
      "355\t train loss: 0.1453 train acc: 59.556% test acc: 58.921% train exerAcc: 52.423% test exerAcc: 51.515%\n",
      "360\t train loss: 0.1443 train acc: 59.556% test acc: 59.323% train exerAcc: 52.863% test exerAcc: 56.061%\n",
      "365\t train loss: 0.1448 train acc: 59.859% test acc: 59.036% train exerAcc: 52.423% test exerAcc: 56.061%\n",
      "370\t train loss: 0.1446 train acc: 59.620% test acc: 58.921% train exerAcc: 52.863% test exerAcc: 54.545%\n",
      "375\t train loss: 0.1454 train acc: 59.700% test acc: 58.864% train exerAcc: 53.744% test exerAcc: 54.545%\n",
      "380\t train loss: 0.1443 train acc: 59.508% test acc: 58.807% train exerAcc: 52.863% test exerAcc: 51.515%\n",
      "385\t train loss: 0.1450 train acc: 59.732% test acc: 58.921% train exerAcc: 51.982% test exerAcc: 53.030%\n",
      "390\t train loss: 0.1449 train acc: 59.540% test acc: 59.151% train exerAcc: 53.304% test exerAcc: 56.061%\n",
      "395\t train loss: 0.1446 train acc: 59.524% test acc: 59.208% train exerAcc: 52.863% test exerAcc: 56.061%\n",
      "399\t train loss: 0.1447 train acc: 59.795% test acc: 58.921% train exerAcc: 53.304% test exerAcc: 54.545%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.2002 train acc: 29.361% test acc: 30.345% train exerAcc: 34.855% test exerAcc: 34.615%\n",
      "5\t train loss: 0.1863 train acc: 42.741% test acc: 42.231% train exerAcc: 45.228% test exerAcc: 51.923%\n",
      "10\t train loss: 0.1764 train acc: 57.691% test acc: 53.453% train exerAcc: 48.548% test exerAcc: 50.000%\n",
      "15\t train loss: 0.1680 train acc: 58.229% test acc: 53.984% train exerAcc: 48.963% test exerAcc: 50.000%\n",
      "20\t train loss: 0.1639 train acc: 58.322% test acc: 54.449% train exerAcc: 48.963% test exerAcc: 50.000%\n",
      "25\t train loss: 0.1607 train acc: 58.614% test acc: 55.378% train exerAcc: 48.963% test exerAcc: 50.000%\n",
      "30\t train loss: 0.1579 train acc: 58.614% test acc: 55.179% train exerAcc: 50.622% test exerAcc: 50.000%\n",
      "35\t train loss: 0.1567 train acc: 58.676% test acc: 55.710% train exerAcc: 50.622% test exerAcc: 50.000%\n",
      "40\t train loss: 0.1546 train acc: 58.784% test acc: 55.777% train exerAcc: 51.452% test exerAcc: 50.000%\n",
      "45\t train loss: 0.1532 train acc: 58.953% test acc: 55.910% train exerAcc: 51.452% test exerAcc: 50.000%\n",
      "50\t train loss: 0.1532 train acc: 58.999% test acc: 55.578% train exerAcc: 51.452% test exerAcc: 50.000%\n",
      "55\t train loss: 0.1517 train acc: 59.061% test acc: 55.644% train exerAcc: 51.452% test exerAcc: 50.000%\n",
      "60\t train loss: 0.1509 train acc: 59.092% test acc: 55.777% train exerAcc: 51.867% test exerAcc: 50.000%\n",
      "65\t train loss: 0.1505 train acc: 59.076% test acc: 56.175% train exerAcc: 51.452% test exerAcc: 50.000%\n",
      "70\t train loss: 0.1497 train acc: 59.184% test acc: 55.843% train exerAcc: 51.452% test exerAcc: 46.154%\n",
      "75\t train loss: 0.1501 train acc: 59.230% test acc: 55.777% train exerAcc: 51.452% test exerAcc: 48.077%\n",
      "80\t train loss: 0.1487 train acc: 59.369% test acc: 55.644% train exerAcc: 52.282% test exerAcc: 46.154%\n",
      "85\t train loss: 0.1489 train acc: 59.569% test acc: 56.042% train exerAcc: 53.112% test exerAcc: 53.846%\n",
      "90\t train loss: 0.1482 train acc: 59.615% test acc: 55.976% train exerAcc: 53.942% test exerAcc: 53.846%\n",
      "95\t train loss: 0.1482 train acc: 59.600% test acc: 55.843% train exerAcc: 52.282% test exerAcc: 53.846%\n",
      "100\t train loss: 0.1477 train acc: 59.677% test acc: 56.375% train exerAcc: 52.282% test exerAcc: 53.846%\n",
      "105\t train loss: 0.1472 train acc: 59.707% test acc: 55.910% train exerAcc: 53.112% test exerAcc: 55.769%\n",
      "110\t train loss: 0.1468 train acc: 59.784% test acc: 56.175% train exerAcc: 51.867% test exerAcc: 55.769%\n",
      "115\t train loss: 0.1464 train acc: 59.738% test acc: 56.042% train exerAcc: 52.282% test exerAcc: 51.923%\n",
      "120\t train loss: 0.1464 train acc: 59.723% test acc: 56.175% train exerAcc: 52.697% test exerAcc: 53.846%\n",
      "125\t train loss: 0.1467 train acc: 59.738% test acc: 56.441% train exerAcc: 52.697% test exerAcc: 53.846%\n",
      "130\t train loss: 0.1456 train acc: 59.877% test acc: 56.109% train exerAcc: 52.282% test exerAcc: 50.000%\n",
      "135\t train loss: 0.1465 train acc: 59.831% test acc: 56.441% train exerAcc: 52.282% test exerAcc: 53.846%\n",
      "140\t train loss: 0.1451 train acc: 59.831% test acc: 56.441% train exerAcc: 52.697% test exerAcc: 53.846%\n",
      "145\t train loss: 0.1455 train acc: 59.846% test acc: 56.308% train exerAcc: 53.527% test exerAcc: 55.769%\n",
      "150\t train loss: 0.1457 train acc: 59.861% test acc: 56.109% train exerAcc: 53.112% test exerAcc: 51.923%\n",
      "155\t train loss: 0.1456 train acc: 59.938% test acc: 56.507% train exerAcc: 53.527% test exerAcc: 53.846%\n",
      "160\t train loss: 0.1456 train acc: 60.092% test acc: 56.507% train exerAcc: 53.527% test exerAcc: 51.923%\n",
      "165\t train loss: 0.1453 train acc: 60.123% test acc: 56.308% train exerAcc: 53.942% test exerAcc: 51.923%\n",
      "170\t train loss: 0.1455 train acc: 60.293% test acc: 56.375% train exerAcc: 54.772% test exerAcc: 51.923%\n",
      "175\t train loss: 0.1454 train acc: 60.462% test acc: 56.375% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "180\t train loss: 0.1451 train acc: 60.431% test acc: 56.242% train exerAcc: 54.772% test exerAcc: 48.077%\n",
      "185\t train loss: 0.1452 train acc: 60.400% test acc: 56.109% train exerAcc: 54.357% test exerAcc: 50.000%\n",
      "190\t train loss: 0.1445 train acc: 60.293% test acc: 55.976% train exerAcc: 55.187% test exerAcc: 50.000%\n",
      "195\t train loss: 0.1442 train acc: 60.339% test acc: 56.242% train exerAcc: 55.187% test exerAcc: 51.923%\n",
      "200\t train loss: 0.1442 train acc: 60.370% test acc: 56.175% train exerAcc: 54.357% test exerAcc: 50.000%\n",
      "205\t train loss: 0.1440 train acc: 60.431% test acc: 55.910% train exerAcc: 53.942% test exerAcc: 50.000%\n",
      "210\t train loss: 0.1444 train acc: 60.416% test acc: 56.175% train exerAcc: 53.527% test exerAcc: 50.000%\n",
      "215\t train loss: 0.1450 train acc: 60.339% test acc: 56.308% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "220\t train loss: 0.1439 train acc: 60.323% test acc: 56.308% train exerAcc: 55.187% test exerAcc: 51.923%\n",
      "225\t train loss: 0.1443 train acc: 60.308% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 50.000%\n",
      "230\t train loss: 0.1437 train acc: 60.462% test acc: 55.976% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "235\t train loss: 0.1433 train acc: 60.477% test acc: 56.042% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "240\t train loss: 0.1440 train acc: 60.431% test acc: 56.042% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "245\t train loss: 0.1441 train acc: 60.400% test acc: 56.042% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "250\t train loss: 0.1443 train acc: 60.354% test acc: 55.777% train exerAcc: 53.527% test exerAcc: 50.000%\n",
      "255\t train loss: 0.1441 train acc: 60.431% test acc: 55.976% train exerAcc: 55.602% test exerAcc: 50.000%\n",
      "260\t train loss: 0.1438 train acc: 60.385% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "265\t train loss: 0.1438 train acc: 60.446% test acc: 55.710% train exerAcc: 55.602% test exerAcc: 50.000%\n",
      "270\t train loss: 0.1438 train acc: 60.477% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "275\t train loss: 0.1430 train acc: 60.477% test acc: 56.042% train exerAcc: 55.187% test exerAcc: 51.923%\n",
      "280\t train loss: 0.1435 train acc: 60.416% test acc: 55.976% train exerAcc: 56.432% test exerAcc: 51.923%\n",
      "285\t train loss: 0.1433 train acc: 60.354% test acc: 55.843% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "290\t train loss: 0.1432 train acc: 60.400% test acc: 55.910% train exerAcc: 55.602% test exerAcc: 51.923%\n",
      "295\t train loss: 0.1435 train acc: 60.416% test acc: 55.910% train exerAcc: 56.432% test exerAcc: 50.000%\n",
      "300\t train loss: 0.1431 train acc: 60.416% test acc: 55.910% train exerAcc: 56.432% test exerAcc: 51.923%\n",
      "305\t train loss: 0.1434 train acc: 60.370% test acc: 55.777% train exerAcc: 54.772% test exerAcc: 51.923%\n",
      "310\t train loss: 0.1433 train acc: 60.477% test acc: 55.777% train exerAcc: 55.187% test exerAcc: 50.000%\n",
      "315\t train loss: 0.1437 train acc: 60.446% test acc: 55.843% train exerAcc: 55.602% test exerAcc: 51.923%\n",
      "320\t train loss: 0.1431 train acc: 60.477% test acc: 55.843% train exerAcc: 56.432% test exerAcc: 51.923%\n",
      "325\t train loss: 0.1430 train acc: 60.354% test acc: 55.910% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "330\t train loss: 0.1431 train acc: 60.323% test acc: 55.777% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "335\t train loss: 0.1434 train acc: 60.370% test acc: 55.843% train exerAcc: 55.602% test exerAcc: 50.000%\n",
      "340\t train loss: 0.1434 train acc: 60.416% test acc: 55.710% train exerAcc: 55.602% test exerAcc: 50.000%\n",
      "345\t train loss: 0.1431 train acc: 60.339% test acc: 55.910% train exerAcc: 55.187% test exerAcc: 51.923%\n",
      "350\t train loss: 0.1431 train acc: 60.354% test acc: 55.777% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "355\t train loss: 0.1435 train acc: 60.308% test acc: 55.843% train exerAcc: 55.187% test exerAcc: 50.000%\n",
      "360\t train loss: 0.1429 train acc: 60.339% test acc: 55.578% train exerAcc: 56.432% test exerAcc: 48.077%\n",
      "365\t train loss: 0.1431 train acc: 60.339% test acc: 55.644% train exerAcc: 56.432% test exerAcc: 50.000%\n",
      "370\t train loss: 0.1433 train acc: 60.370% test acc: 55.644% train exerAcc: 56.846% test exerAcc: 50.000%\n",
      "375\t train loss: 0.1428 train acc: 60.354% test acc: 55.710% train exerAcc: 55.602% test exerAcc: 50.000%\n",
      "380\t train loss: 0.1428 train acc: 60.262% test acc: 55.710% train exerAcc: 56.017% test exerAcc: 50.000%\n",
      "385\t train loss: 0.1428 train acc: 60.277% test acc: 55.644% train exerAcc: 54.357% test exerAcc: 50.000%\n",
      "390\t train loss: 0.1424 train acc: 60.277% test acc: 55.445% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "395\t train loss: 0.1434 train acc: 60.400% test acc: 55.644% train exerAcc: 56.017% test exerAcc: 51.923%\n",
      "399\t train loss: 0.1426 train acc: 60.354% test acc: 55.578% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.1893 train acc: 52.208% test acc: 54.029% train exerAcc: 48.261% test exerAcc: 52.381%\n",
      "5\t train loss: 0.1663 train acc: 53.340% test acc: 56.105% train exerAcc: 48.261% test exerAcc: 49.206%\n",
      "10\t train loss: 0.1606 train acc: 54.408% test acc: 57.021% train exerAcc: 47.391% test exerAcc: 52.381%\n",
      "15\t train loss: 0.1563 train acc: 57.159% test acc: 59.890% train exerAcc: 47.391% test exerAcc: 52.381%\n",
      "20\t train loss: 0.1542 train acc: 57.536% test acc: 59.585% train exerAcc: 49.130% test exerAcc: 50.794%\n",
      "25\t train loss: 0.1521 train acc: 57.756% test acc: 59.585% train exerAcc: 51.304% test exerAcc: 44.444%\n",
      "30\t train loss: 0.1506 train acc: 57.882% test acc: 60.195% train exerAcc: 52.609% test exerAcc: 52.381%\n",
      "35\t train loss: 0.1493 train acc: 58.196% test acc: 60.317% train exerAcc: 54.783% test exerAcc: 53.968%\n",
      "40\t train loss: 0.1468 train acc: 59.107% test acc: 60.501% train exerAcc: 57.391% test exerAcc: 49.206%\n",
      "45\t train loss: 0.1439 train acc: 59.846% test acc: 60.623% train exerAcc: 61.304% test exerAcc: 50.794%\n",
      "50\t train loss: 0.1430 train acc: 60.600% test acc: 60.134% train exerAcc: 60.870% test exerAcc: 50.794%\n",
      "55\t train loss: 0.1414 train acc: 61.669% test acc: 59.524% train exerAcc: 63.478% test exerAcc: 49.206%\n",
      "60\t train loss: 0.1403 train acc: 62.188% test acc: 60.073% train exerAcc: 64.783% test exerAcc: 50.794%\n",
      "65\t train loss: 0.1389 train acc: 62.565% test acc: 60.256% train exerAcc: 66.087% test exerAcc: 49.206%\n",
      "70\t train loss: 0.1376 train acc: 62.706% test acc: 60.012% train exerAcc: 66.087% test exerAcc: 50.794%\n",
      "75\t train loss: 0.1362 train acc: 63.366% test acc: 60.989% train exerAcc: 68.261% test exerAcc: 49.206%\n",
      "80\t train loss: 0.1346 train acc: 63.932% test acc: 59.585% train exerAcc: 64.783% test exerAcc: 52.381%\n",
      "85\t train loss: 0.1333 train acc: 64.341% test acc: 59.524% train exerAcc: 66.087% test exerAcc: 52.381%\n",
      "90\t train loss: 0.1330 train acc: 64.419% test acc: 60.073% train exerAcc: 67.391% test exerAcc: 53.968%\n",
      "95\t train loss: 0.1315 train acc: 64.875% test acc: 58.852% train exerAcc: 65.652% test exerAcc: 53.968%\n",
      "100\t train loss: 0.1307 train acc: 65.692% test acc: 59.219% train exerAcc: 70.870% test exerAcc: 55.556%\n",
      "105\t train loss: 0.1295 train acc: 65.897% test acc: 60.012% train exerAcc: 70.435% test exerAcc: 55.556%\n",
      "110\t train loss: 0.1285 train acc: 66.289% test acc: 59.219% train exerAcc: 73.043% test exerAcc: 53.968%\n",
      "115\t train loss: 0.1265 train acc: 66.698% test acc: 59.096% train exerAcc: 70.870% test exerAcc: 53.968%\n",
      "120\t train loss: 0.1255 train acc: 67.295% test acc: 59.280% train exerAcc: 72.174% test exerAcc: 52.381%\n",
      "125\t train loss: 0.1241 train acc: 67.594% test acc: 59.035% train exerAcc: 73.478% test exerAcc: 55.556%\n",
      "130\t train loss: 0.1240 train acc: 67.531% test acc: 57.509% train exerAcc: 73.478% test exerAcc: 58.730%\n",
      "135\t train loss: 0.1236 train acc: 67.782% test acc: 60.134% train exerAcc: 76.087% test exerAcc: 53.968%\n",
      "140\t train loss: 0.1213 train acc: 68.537% test acc: 59.096% train exerAcc: 74.783% test exerAcc: 55.556%\n",
      "145\t train loss: 0.1200 train acc: 69.008% test acc: 59.646% train exerAcc: 78.696% test exerAcc: 55.556%\n",
      "150\t train loss: 0.1193 train acc: 69.637% test acc: 59.829% train exerAcc: 81.304% test exerAcc: 57.143%\n",
      "155\t train loss: 0.1173 train acc: 69.998% test acc: 58.608% train exerAcc: 83.043% test exerAcc: 58.730%\n",
      "160\t train loss: 0.1158 train acc: 70.501% test acc: 59.402% train exerAcc: 83.478% test exerAcc: 55.556%\n",
      "165\t train loss: 0.1147 train acc: 70.737% test acc: 58.242% train exerAcc: 80.435% test exerAcc: 49.206%\n",
      "170\t train loss: 0.1135 train acc: 71.413% test acc: 57.692% train exerAcc: 86.522% test exerAcc: 50.794%\n",
      "175\t train loss: 0.1125 train acc: 72.089% test acc: 59.829% train exerAcc: 89.565% test exerAcc: 55.556%\n",
      "180\t train loss: 0.1125 train acc: 72.214% test acc: 58.852% train exerAcc: 89.130% test exerAcc: 55.556%\n",
      "185\t train loss: 0.1103 train acc: 72.623% test acc: 58.547% train exerAcc: 87.826% test exerAcc: 55.556%\n",
      "190\t train loss: 0.1083 train acc: 72.372% test acc: 58.181% train exerAcc: 87.391% test exerAcc: 50.794%\n",
      "195\t train loss: 0.1087 train acc: 72.749% test acc: 58.669% train exerAcc: 90.435% test exerAcc: 50.794%\n",
      "199\t train loss: 0.1065 train acc: 73.692% test acc: 58.303% train exerAcc: 87.826% test exerAcc: 52.381%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.2029 train acc: 53.020% test acc: 52.897% train exerAcc: 48.458% test exerAcc: 48.485%\n",
      "5\t train loss: 0.1579 train acc: 58.213% test acc: 58.118% train exerAcc: 48.458% test exerAcc: 48.485%\n",
      "10\t train loss: 0.1505 train acc: 58.549% test acc: 57.487% train exerAcc: 48.458% test exerAcc: 48.485%\n",
      "15\t train loss: 0.1482 train acc: 58.853% test acc: 58.061% train exerAcc: 50.220% test exerAcc: 56.061%\n",
      "20\t train loss: 0.1469 train acc: 59.556% test acc: 59.151% train exerAcc: 52.423% test exerAcc: 56.061%\n",
      "25\t train loss: 0.1441 train acc: 60.099% test acc: 59.552% train exerAcc: 55.066% test exerAcc: 57.576%\n",
      "30\t train loss: 0.1428 train acc: 61.074% test acc: 59.667% train exerAcc: 60.352% test exerAcc: 56.061%\n",
      "35\t train loss: 0.1400 train acc: 61.106% test acc: 59.552% train exerAcc: 57.709% test exerAcc: 51.515%\n",
      "40\t train loss: 0.1381 train acc: 62.464% test acc: 59.495% train exerAcc: 61.233% test exerAcc: 54.545%\n",
      "45\t train loss: 0.1366 train acc: 63.327% test acc: 58.979% train exerAcc: 60.352% test exerAcc: 54.545%\n",
      "50\t train loss: 0.1348 train acc: 64.286% test acc: 59.495% train exerAcc: 61.233% test exerAcc: 53.030%\n",
      "55\t train loss: 0.1329 train acc: 64.765% test acc: 60.643% train exerAcc: 63.877% test exerAcc: 51.515%\n",
      "60\t train loss: 0.1311 train acc: 65.516% test acc: 60.126% train exerAcc: 64.758% test exerAcc: 53.030%\n",
      "65\t train loss: 0.1296 train acc: 65.005% test acc: 60.356% train exerAcc: 66.520% test exerAcc: 51.515%\n",
      "70\t train loss: 0.1272 train acc: 66.667% test acc: 60.011% train exerAcc: 68.282% test exerAcc: 53.030%\n",
      "75\t train loss: 0.1262 train acc: 67.721% test acc: 59.208% train exerAcc: 70.044% test exerAcc: 53.030%\n",
      "80\t train loss: 0.1226 train acc: 68.504% test acc: 58.520% train exerAcc: 71.806% test exerAcc: 53.030%\n",
      "85\t train loss: 0.1205 train acc: 70.006% test acc: 59.552% train exerAcc: 73.128% test exerAcc: 54.545%\n",
      "90\t train loss: 0.1181 train acc: 69.815% test acc: 58.348% train exerAcc: 70.925% test exerAcc: 50.000%\n",
      "95\t train loss: 0.1183 train acc: 70.821% test acc: 57.717% train exerAcc: 74.449% test exerAcc: 51.515%\n",
      "100\t train loss: 0.1138 train acc: 71.588% test acc: 58.061% train exerAcc: 75.771% test exerAcc: 50.000%\n",
      "105\t train loss: 0.1112 train acc: 72.531% test acc: 58.348% train exerAcc: 77.533% test exerAcc: 45.455%\n",
      "110\t train loss: 0.1087 train acc: 73.490% test acc: 57.831% train exerAcc: 76.652% test exerAcc: 42.424%\n",
      "115\t train loss: 0.1063 train acc: 74.017% test acc: 57.717% train exerAcc: 78.855% test exerAcc: 45.455%\n",
      "120\t train loss: 0.1046 train acc: 75.056% test acc: 57.602% train exerAcc: 78.855% test exerAcc: 42.424%\n",
      "125\t train loss: 0.1017 train acc: 75.919% test acc: 56.110% train exerAcc: 80.617% test exerAcc: 42.424%\n",
      "130\t train loss: 0.1019 train acc: 75.903% test acc: 56.741% train exerAcc: 82.819% test exerAcc: 40.909%\n",
      "135\t train loss: 0.1039 train acc: 75.328% test acc: 57.831% train exerAcc: 77.093% test exerAcc: 40.909%\n",
      "140\t train loss: 0.1005 train acc: 75.344% test acc: 56.913% train exerAcc: 81.498% test exerAcc: 46.970%\n",
      "145\t train loss: 0.1007 train acc: 77.788% test acc: 57.430% train exerAcc: 84.582% test exerAcc: 42.424%\n",
      "150\t train loss: 0.0944 train acc: 77.613% test acc: 57.544% train exerAcc: 84.582% test exerAcc: 43.939%\n",
      "155\t train loss: 0.0936 train acc: 78.859% test acc: 57.315% train exerAcc: 85.463% test exerAcc: 46.970%\n",
      "160\t train loss: 0.0902 train acc: 79.227% test acc: 57.717% train exerAcc: 88.106% test exerAcc: 43.939%\n",
      "165\t train loss: 0.0894 train acc: 80.361% test acc: 57.544% train exerAcc: 88.987% test exerAcc: 45.455%\n",
      "170\t train loss: 0.0866 train acc: 80.361% test acc: 55.881% train exerAcc: 87.665% test exerAcc: 45.455%\n",
      "175\t train loss: 0.0870 train acc: 80.920% test acc: 56.225% train exerAcc: 88.106% test exerAcc: 45.455%\n",
      "180\t train loss: 0.0849 train acc: 79.770% test acc: 57.085% train exerAcc: 85.022% test exerAcc: 40.909%\n",
      "185\t train loss: 0.0841 train acc: 81.655% test acc: 55.594% train exerAcc: 88.106% test exerAcc: 43.939%\n",
      "190\t train loss: 0.0841 train acc: 81.224% test acc: 54.561% train exerAcc: 87.225% test exerAcc: 50.000%\n",
      "195\t train loss: 0.0837 train acc: 81.288% test acc: 55.651% train exerAcc: 88.987% test exerAcc: 37.879%\n",
      "199\t train loss: 0.0815 train acc: 79.179% test acc: 54.159% train exerAcc: 81.938% test exerAcc: 43.939%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'q1_cat' 'q1_cat']\n",
      "0\t train loss: 0.2024 train acc: 54.273% test acc: 52.058% train exerAcc: 47.303% test exerAcc: 46.154%\n",
      "5\t train loss: 0.1612 train acc: 54.226% test acc: 52.258% train exerAcc: 48.133% test exerAcc: 50.000%\n",
      "10\t train loss: 0.1549 train acc: 55.874% test acc: 51.726% train exerAcc: 48.133% test exerAcc: 50.000%\n",
      "15\t train loss: 0.1524 train acc: 58.507% test acc: 55.843% train exerAcc: 48.133% test exerAcc: 50.000%\n",
      "20\t train loss: 0.1494 train acc: 58.799% test acc: 53.851% train exerAcc: 51.037% test exerAcc: 44.231%\n",
      "25\t train loss: 0.1466 train acc: 59.707% test acc: 55.046% train exerAcc: 52.282% test exerAcc: 50.000%\n",
      "30\t train loss: 0.1433 train acc: 60.801% test acc: 55.710% train exerAcc: 54.772% test exerAcc: 53.846%\n",
      "35\t train loss: 0.1408 train acc: 60.970% test acc: 56.042% train exerAcc: 56.846% test exerAcc: 48.077%\n",
      "40\t train loss: 0.1400 train acc: 61.370% test acc: 56.839% train exerAcc: 59.336% test exerAcc: 50.000%\n",
      "45\t train loss: 0.1379 train acc: 62.463% test acc: 55.710% train exerAcc: 57.261% test exerAcc: 59.615%\n",
      "50\t train loss: 0.1361 train acc: 63.326% test acc: 54.449% train exerAcc: 58.506% test exerAcc: 57.692%\n",
      "55\t train loss: 0.1339 train acc: 63.695% test acc: 55.179% train exerAcc: 54.357% test exerAcc: 55.769%\n",
      "60\t train loss: 0.1336 train acc: 63.880% test acc: 54.582% train exerAcc: 58.091% test exerAcc: 42.308%\n",
      "65\t train loss: 0.1313 train acc: 65.189% test acc: 55.179% train exerAcc: 58.091% test exerAcc: 59.615%\n",
      "70\t train loss: 0.1297 train acc: 65.820% test acc: 53.851% train exerAcc: 62.656% test exerAcc: 53.846%\n",
      "75\t train loss: 0.1286 train acc: 66.667% test acc: 53.320% train exerAcc: 61.826% test exerAcc: 50.000%\n",
      "80\t train loss: 0.1259 train acc: 67.082% test acc: 54.648% train exerAcc: 66.390% test exerAcc: 50.000%\n",
      "85\t train loss: 0.1241 train acc: 67.883% test acc: 53.586% train exerAcc: 64.730% test exerAcc: 50.000%\n",
      "90\t train loss: 0.1242 train acc: 68.406% test acc: 53.386% train exerAcc: 66.805% test exerAcc: 46.154%\n",
      "95\t train loss: 0.1220 train acc: 68.637% test acc: 53.187% train exerAcc: 67.635% test exerAcc: 42.308%\n",
      "100\t train loss: 0.1192 train acc: 69.192% test acc: 53.453% train exerAcc: 69.710% test exerAcc: 44.231%\n",
      "105\t train loss: 0.1169 train acc: 70.701% test acc: 52.125% train exerAcc: 74.274% test exerAcc: 42.308%\n",
      "110\t train loss: 0.1142 train acc: 71.193% test acc: 52.324% train exerAcc: 75.104% test exerAcc: 42.308%\n",
      "115\t train loss: 0.1125 train acc: 72.317% test acc: 51.859% train exerAcc: 79.253% test exerAcc: 42.308%\n",
      "120\t train loss: 0.1125 train acc: 71.701% test acc: 49.801% train exerAcc: 80.083% test exerAcc: 44.231%\n",
      "125\t train loss: 0.1126 train acc: 73.102% test acc: 52.523% train exerAcc: 80.913% test exerAcc: 42.308%\n",
      "130\t train loss: 0.1084 train acc: 73.764% test acc: 49.934% train exerAcc: 84.232% test exerAcc: 40.385%\n",
      "135\t train loss: 0.1050 train acc: 74.765% test acc: 49.734% train exerAcc: 83.402% test exerAcc: 44.231%\n",
      "140\t train loss: 0.1028 train acc: 75.412% test acc: 49.070% train exerAcc: 85.477% test exerAcc: 34.615%\n",
      "145\t train loss: 0.1004 train acc: 76.305% test acc: 49.535% train exerAcc: 89.212% test exerAcc: 38.462%\n",
      "150\t train loss: 0.0979 train acc: 76.520% test acc: 48.938% train exerAcc: 88.382% test exerAcc: 38.462%\n",
      "155\t train loss: 0.0959 train acc: 77.259% test acc: 49.336% train exerAcc: 87.137% test exerAcc: 36.538%\n",
      "160\t train loss: 0.0942 train acc: 77.198% test acc: 50.531% train exerAcc: 88.797% test exerAcc: 40.385%\n",
      "165\t train loss: 0.0932 train acc: 77.706% test acc: 49.070% train exerAcc: 87.552% test exerAcc: 40.385%\n",
      "170\t train loss: 0.0921 train acc: 78.229% test acc: 49.336% train exerAcc: 87.552% test exerAcc: 30.769%\n",
      "175\t train loss: 0.0937 train acc: 78.891% test acc: 50.000% train exerAcc: 90.041% test exerAcc: 32.692%\n",
      "180\t train loss: 0.0918 train acc: 77.937% test acc: 48.141% train exerAcc: 87.967% test exerAcc: 34.615%\n",
      "185\t train loss: 0.0910 train acc: 79.230% test acc: 49.734% train exerAcc: 88.797% test exerAcc: 36.538%\n",
      "190\t train loss: 0.0874 train acc: 79.723% test acc: 49.004% train exerAcc: 92.116% test exerAcc: 36.538%\n",
      "195\t train loss: 0.0858 train acc: 80.385% test acc: 50.133% train exerAcc: 90.041% test exerAcc: 42.308%\n",
      "199\t train loss: 0.0854 train acc: 80.246% test acc: 49.070% train exerAcc: 91.701% test exerAcc: 36.538%\n",
      "268 537\n",
      "(12912, 204) (12912, 4) 204\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'qids_q1'\n",
      " 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'AccExercise' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25792\\67489863.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m                                     })\n\u001b[0;32m     52\u001b[0m                                 \u001b[1;31m# torch.autograd.set_detect_anomaly(True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                                 \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mrep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mrep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"_kw\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;31m# results = self.evaluate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mtmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[0mtest_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{e}\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"train loss: {lh[0]:.4f}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"train acc: {metrics[labels.index('Acc')]:.3%}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"test acc: {tmetrics[labels.index('Acc')]:.3%}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"train exerAcc: {metrics[labels.index('AccExercise')]:.3%}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"test exerAcc: {tmetrics[labels.index('AccExercise')]:.3%}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msched\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscheds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0msched\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'AccExercise' is not in list"
     ]
    }
   ],
   "source": [
    "knowSched = [30, 180]\n",
    "physSched = [30, 100, 180]\n",
    "conSched = [100]\n",
    "\n",
    "for respond_perc in [.5]:\n",
    "# for respond_perc in [.75, .25]:\n",
    "    # for estate, include_state in [(True, True)]:\n",
    "    for splitQ, splitM in [(True, True), (True, False), (False, True), (False, False)]:\n",
    "    # for splitQ, splitM in [(False, True)]:\n",
    "        for estate, include_state, fullq in [(True, True, True)]:\n",
    "            for fulls, insertpreds in [(False, True)]:\n",
    "                # for model, learning_rate, epochs in [(\"BasicNN\", .0054, 300), (\"LogisticRegressor\", .003, 400), (\"AdaptableLSTM\", .07, 200)]:\n",
    "                for model, learning_rate, epochs in [(\"AdaptableLSTM\", .07, 200)]:\n",
    "                # for model, learning_rate, epochs in [(\"BasicNN\", .0054, 300), (\"LogisticRegressor\", .003, 400)]:\n",
    "                    for smooth, noise in [(0, .07)]:\n",
    "                        for loss_fn in [\"MSELoss\"]:\n",
    "                            test_metrics, train_metrics, adjusted_losses = [], [], []\n",
    "                            for seed in range(3):\n",
    "                                np.random.seed(seed)\n",
    "                                torch.manual_seed(seed)\n",
    "                                e = Experiment(\n",
    "                                    modelSplit = splitM,\n",
    "                                    numValFolds = 5,\n",
    "                                    epochsToUpdateLabelMods = 10,\n",
    "                                    knowSchedule = knowSched,\n",
    "                                    consumpSchedule = conSched,\n",
    "                                    physSchedule = physSched,\n",
    "                                    data_kw={\"minw\": 2,\n",
    "                                            \"maxw\": 31,\n",
    "                                            \"include_state\": include_state,\n",
    "                                            \"include_pid\": False,\n",
    "                                            \"expanded_states\": estate,\n",
    "                                            \"top_respond_perc\": respond_perc,\n",
    "                                             \"full_questionnaire\": fullq,\n",
    "                                             \"full_sequence\": fulls,\n",
    "                                             \"insert_predictions\": insertpreds,\n",
    "                                             \"split_model_features\": splitM,\n",
    "                                             \"split_weekly_questions\": splitQ\n",
    "                                            },\n",
    "                                    model=model,\n",
    "                                    model_kw={\n",
    "                                        \"lossfn\": loss_fn,\n",
    "                                        # \"lossfn\": \"NDCG\",\n",
    "                                        # \"lossfn\": \"CrossEntropyLoss\",\n",
    "                                        \"hidden_size\": 25, \n",
    "                                        \"lr_step_mult\": .9, \n",
    "                                        \"lr_step_epochs\": 60,\n",
    "                                        \"opt_kw\": {\n",
    "                                            \"lr\": learning_rate\n",
    "                                        },\n",
    "                                        \"labelSmoothPerc\": smooth,\n",
    "                                        \"gaussianNoiseStd\": noise,\n",
    "                                        \"splitModel\": splitM,\n",
    "                                        \"splitWeeklyQuestions\": splitQ\n",
    "                                    },\n",
    "                                    train_kw={\n",
    "                                        \"epochs\": epochs,\n",
    "                                        \"n_subj\": 500,\n",
    "                                        \"rec_every\": 5,\n",
    "                                    })\n",
    "                                # torch.autograd.set_detect_anomaly(True)\n",
    "                                report = e.run()\n",
    "\n",
    "\n",
    "\n",
    "                                individual_test_scores, labels = e.report_scores_individual_test()\n",
    "                                individual_train_scores, labels = e.report_scores_individual_train()\n",
    "\n",
    "\n",
    "\n",
    "                                dire = \"./experiment_output/\"\n",
    "                                # dire = \"./TEMP/\"\n",
    "                                fileprefix = f\"{model}LR{learning_rate}Resp{respond_perc}States{int(include_state)}Expanded{int(estate)}Seq{int(fulls)}Pred{int(insertpreds)}Smooth{smooth}Noise{noise}Split{int(splitQ)}{int(splitM)}\"\n",
    "                                np.savetxt(f\"{dire}TRAINMETRICS-{fileprefix}S{seed}.csv\", report[\"train_metrics\"], delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}TESTMETRICS-{fileprefix}S{seed}.csv\", report[\"test_metrics\"], delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}IDVDTESTMETRICS-{fileprefix}S{seed}.csv\", individual_test_scores, delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}IDVDTRAINMETRICS-{fileprefix}S{seed}.csv\", individual_train_scores, delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}TRAINLOSSES-{fileprefix}S{seed}.csv\", report[\"loss\"], delimiter = ',')\n",
    "\n",
    "                                preds1, preds2, preds3 = e.get_class_predictions(False)\n",
    "\n",
    "\n",
    "                                np.savetxt(f\"{dire}TRAINPREDS1-{fileprefix}S{seed}.csv\", preds1, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TRAINPREDS2-{fileprefix}S{seed}.csv\", preds2, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TRAINPREDS3-{fileprefix}S{seed}.csv\", preds3, delimiter = ',')\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds1)\n",
    "                                plt.title(\"Train Predictions for Class 1\")\n",
    "                                plt.savefig(f\"{dire}/img/C1PredsTrain-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds2)\n",
    "                                plt.title(\"Train Predictions for Class 2\")\n",
    "                                plt.savefig(f\"{dire}/img/C2PredsTrain-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds3)\n",
    "                                plt.title(\"Train Predictions for Class 3\")\n",
    "                                plt.savefig(f\"{dire}/img/C3PredsTrain-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                preds1, preds2, preds3 = e.get_class_predictions(True)\n",
    "\n",
    "                                np.savetxt(f\"{dire}TESTPREDS1-{fileprefix}S{seed}.csv\", preds1, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TESTPREDS2-{fileprefix}S{seed}.csv\", preds2, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TESTPREDS3-{fileprefix}S{seed}.csv\", preds3, delimiter = ',')\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds1)\n",
    "                                plt.title(\"Test Predictions for Class 1\")\n",
    "                                plt.savefig(f\"{dire}/img/C1PredsTest-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds2)\n",
    "                                plt.title(\"Test Predictions for Class 2\")\n",
    "                                plt.savefig(f\"{dire}/img/C2PredsTest-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds3)\n",
    "                                plt.title(\"Test Predictions for Class 3\")\n",
    "                                plt.savefig(f\"{dire}/img/C3PredsTest-{fileprefix}S{seed}.png\")\n",
    "\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.scatter(individual_test_scores[:, -1], individual_test_scores[:, labels.index(\"Acc\")])\n",
    "                                plt.title(\"Test Accuracy vs Response Count\")\n",
    "                                plt.savefig(f\"{dire}/img/TestACCvResponse-{fileprefix}S{seed}.png\")\n",
    "                                plt.clf()\n",
    "                                splot = plt.scatter(individual_train_scores[:, -1], individual_train_scores[:, labels.index(\"Acc\")])\n",
    "                                plt.title(\"Train Accuracy vs Response Count\")\n",
    "                                plt.savefig(f\"{dire}/img/TrainACCvResponse-{fileprefix}S{seed}.png\")\n",
    "                                plt.clf()\n",
    "\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"Acc\")], label=\"Train Acc.\")\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"Acc\")], label=\"Test Acc.\")\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"MSE\")], label=\"Train MSE\")\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"MSE\")], label=\"Test MSE\")\n",
    "                                plt.title(\"Train/Test Performance Over Training\")\n",
    "                                plt.legend()\n",
    "                                plt.ylabel(\"Metric\")\n",
    "                                plt.xlabel(\"Training Epoch\")\n",
    "                                plt.savefig(f\"{dire}/img/AccuracyVsEpoch-{fileprefix}S{seed}.png\")\n",
    "                                plt.clf()\n",
    "\n",
    "\n",
    "                                writer = open(f\"{dire}ALOSS-{fileprefix}.csv\", \"a\")\n",
    "                                writer.write(\",\".join([str(loss[0]) for loss in report[\"loss\"]]))\n",
    "                                writer.write(\"\\n\")\n",
    "                                writer.close()\n",
    "                                \n",
    "                                bestResult = np.argmax(report[\"test_metrics\"][:, labels.index(\"Acc\")])\n",
    "\n",
    "                                writer = open(f\"{dire}FINALTRAINMETRICS-{fileprefix}.csv\", \"a\")\n",
    "                                writer.write(\",\".join([str(loss) for loss in report[\"train_metrics\"][bestResult, :]]))\n",
    "                                writer.write(\"\\n\")\n",
    "                                writer.close()\n",
    "\n",
    "                                writer = open(f\"{dire}FINALTESTMETRICS-{fileprefix}.csv\", \"a\")\n",
    "                                writer.write(\",\".join([str(loss) for loss in report[\"test_metrics\"][bestResult, :]]))\n",
    "                                writer.write(\"\\n\")\n",
    "                                writer.close()\n",
    "\n",
    "                                Plotter.training_loss(report, dire)\n",
    "\n",
    "                                plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea7b5b-a82c-4039-b292-11c96933e9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6e339f590beffb2e62e02c6be9b431caf4c76db3ef9baeb9786d6033ee27a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
