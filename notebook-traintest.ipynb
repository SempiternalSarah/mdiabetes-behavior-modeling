{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23791e19-fa29-4321-9a92-62c66249e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "from utils.behavior_data import BehaviorData\n",
    "from visuals import Plotter\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.state_data import StateData\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ccab30-3fb9-4990-8492-e7d81a539a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'response_last_q2' 'response_last_q2'\n",
      " 'response_last_q2' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q2' 'paction_sids_q2'\n",
      " 'paction_sids_q2' 'paction_sids_q2' 'paction_sids_q2' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2'\n",
      " 'pmsg_ids_q2' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1'\n",
      " 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'q1_cat'\n",
      " 'q1_cat' 'q2_cat' 'q2_cat']\n",
      "0\t train loss: 0.1999 train acc: 55.383% test acc: 57.570%\n",
      "5\t train loss: 0.1671 train acc: 55.383% test acc: 57.570%\n",
      "10\t train loss: 0.1619 train acc: 56.058% test acc: 57.448%\n",
      "15\t train loss: 0.1614 train acc: 55.430% test acc: 57.998%\n",
      "20\t train loss: 0.1587 train acc: 56.891% test acc: 58.425%\n",
      "25\t train loss: 0.1576 train acc: 56.593% test acc: 58.181%\n",
      "30\t train loss: 0.1554 train acc: 57.190% test acc: 59.035%\n",
      "35\t train loss: 0.1527 train acc: 58.180% test acc: 59.524%\n",
      "40\t train loss: 0.1514 train acc: 59.154% test acc: 57.998%\n",
      "45\t train loss: 0.1497 train acc: 59.783% test acc: 58.852%\n",
      "50\t train loss: 0.1467 train acc: 60.239% test acc: 59.341%\n",
      "55\t train loss: 0.1453 train acc: 61.229% test acc: 58.852%\n",
      "60\t train loss: 0.1441 train acc: 61.905% test acc: 59.524%\n",
      "65\t train loss: 0.1418 train acc: 61.748% test acc: 56.716%\n",
      "70\t train loss: 0.1412 train acc: 62.502% test acc: 59.768%\n",
      "75\t train loss: 0.1393 train acc: 62.911% test acc: 57.631%\n",
      "80\t train loss: 0.1387 train acc: 63.979% test acc: 58.730%\n",
      "85\t train loss: 0.1364 train acc: 64.624% test acc: 57.998%\n",
      "90\t train loss: 0.1346 train acc: 65.174% test acc: 58.669%\n",
      "95\t train loss: 0.1326 train acc: 65.834% test acc: 56.960%\n",
      "100\t train loss: 0.1308 train acc: 66.258% test acc: 56.777%\n",
      "105\t train loss: 0.1302 train acc: 66.682% test acc: 56.471%\n",
      "110\t train loss: 0.1286 train acc: 67.264% test acc: 56.838%\n",
      "115\t train loss: 0.1275 train acc: 67.154% test acc: 57.753%\n",
      "120\t train loss: 0.1254 train acc: 67.641% test acc: 57.998%\n",
      "125\t train loss: 0.1254 train acc: 67.217% test acc: 58.547%\n",
      "130\t train loss: 0.1289 train acc: 66.840% test acc: 53.419%\n",
      "135\t train loss: 0.1229 train acc: 67.877% test acc: 54.762%\n",
      "140\t train loss: 0.1231 train acc: 69.150% test acc: 54.090%\n",
      "145\t train loss: 0.1216 train acc: 69.574% test acc: 55.128%\n",
      "150\t train loss: 0.1202 train acc: 69.936% test acc: 55.006%\n",
      "155\t train loss: 0.1190 train acc: 70.611% test acc: 55.678%\n",
      "160\t train loss: 0.1179 train acc: 71.256% test acc: 54.823%\n",
      "165\t train loss: 0.1166 train acc: 71.696% test acc: 55.006%\n",
      "170\t train loss: 0.1156 train acc: 72.246% test acc: 55.250%\n",
      "175\t train loss: 0.1142 train acc: 72.262% test acc: 54.701%\n",
      "180\t train loss: 0.1145 train acc: 65.818% test acc: 50.427%\n",
      "185\t train loss: 0.1254 train acc: 71.476% test acc: 55.006%\n",
      "190\t train loss: 0.1188 train acc: 71.979% test acc: 55.617%\n",
      "195\t train loss: 0.1152 train acc: 72.544% test acc: 54.640%\n",
      "199\t train loss: 0.1120 train acc: 73.063% test acc: 53.846%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'response_last_q2' 'response_last_q2'\n",
      " 'response_last_q2' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q2' 'paction_sids_q2'\n",
      " 'paction_sids_q2' 'paction_sids_q2' 'paction_sids_q2' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2'\n",
      " 'pmsg_ids_q2' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1'\n",
      " 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'q1_cat'\n",
      " 'q1_cat' 'q2_cat' 'q2_cat']\n",
      "0\t train loss: 0.1958 train acc: 55.817% test acc: 56.110%\n",
      "5\t train loss: 0.1641 train acc: 55.641% test acc: 56.340%\n",
      "10\t train loss: 0.1603 train acc: 56.088% test acc: 56.569%\n",
      "15\t train loss: 0.1560 train acc: 57.478% test acc: 57.430%\n",
      "20\t train loss: 0.1531 train acc: 58.166% test acc: 57.831%\n",
      "25\t train loss: 0.1524 train acc: 58.917% test acc: 58.462%\n",
      "30\t train loss: 0.1499 train acc: 59.556% test acc: 57.831%\n",
      "35\t train loss: 0.1488 train acc: 60.035% test acc: 57.774%\n",
      "40\t train loss: 0.1469 train acc: 60.099% test acc: 57.258%\n",
      "45\t train loss: 0.1443 train acc: 60.786% test acc: 57.258%\n",
      "50\t train loss: 0.1438 train acc: 61.393% test acc: 56.684%\n",
      "55\t train loss: 0.1422 train acc: 62.464% test acc: 56.340%\n",
      "60\t train loss: 0.1414 train acc: 62.544% test acc: 56.168%\n",
      "65\t train loss: 0.1422 train acc: 62.144% test acc: 57.372%\n",
      "70\t train loss: 0.1406 train acc: 63.758% test acc: 57.659%\n",
      "75\t train loss: 0.1386 train acc: 64.158% test acc: 57.085%\n",
      "80\t train loss: 0.1371 train acc: 64.813% test acc: 56.053%\n",
      "85\t train loss: 0.1348 train acc: 65.213% test acc: 56.168%\n",
      "90\t train loss: 0.1344 train acc: 65.660% test acc: 55.536%\n",
      "95\t train loss: 0.1328 train acc: 66.219% test acc: 55.995%\n",
      "100\t train loss: 0.1316 train acc: 66.715% test acc: 56.053%\n",
      "105\t train loss: 0.1301 train acc: 67.002% test acc: 55.536%\n",
      "110\t train loss: 0.1293 train acc: 67.274% test acc: 55.766%\n",
      "115\t train loss: 0.1275 train acc: 67.689% test acc: 55.823%\n",
      "120\t train loss: 0.1263 train acc: 68.217% test acc: 55.192%\n",
      "125\t train loss: 0.1252 train acc: 68.616% test acc: 55.709%\n",
      "130\t train loss: 0.1244 train acc: 68.169% test acc: 55.192%\n",
      "135\t train loss: 0.1229 train acc: 69.671% test acc: 55.192%\n",
      "140\t train loss: 0.1208 train acc: 69.926% test acc: 54.848%\n",
      "145\t train loss: 0.1207 train acc: 70.550% test acc: 55.020%\n",
      "150\t train loss: 0.1200 train acc: 71.365% test acc: 54.389%\n",
      "155\t train loss: 0.1173 train acc: 71.588% test acc: 54.561%\n",
      "160\t train loss: 0.1162 train acc: 71.972% test acc: 53.069%\n",
      "165\t train loss: 0.1163 train acc: 71.924% test acc: 53.012%\n",
      "170\t train loss: 0.1156 train acc: 72.563% test acc: 53.242%\n",
      "175\t train loss: 0.1131 train acc: 73.282% test acc: 53.701%\n",
      "180\t train loss: 0.1122 train acc: 73.346% test acc: 53.643%\n",
      "185\t train loss: 0.1128 train acc: 73.778% test acc: 54.217%\n",
      "190\t train loss: 0.1106 train acc: 73.122% test acc: 55.077%\n",
      "195\t train loss: 0.1105 train acc: 74.385% test acc: 53.758%\n",
      "199\t train loss: 0.1097 train acc: 74.624% test acc: 53.069%\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_q1' 'response_last_q1'\n",
      " 'response_last_q1' 'response_last_q2' 'response_last_q2'\n",
      " 'response_last_q2' 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q1'\n",
      " 'paction_sids_q1' 'paction_sids_q1' 'paction_sids_q2' 'paction_sids_q2'\n",
      " 'paction_sids_q2' 'paction_sids_q2' 'paction_sids_q2' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1' 'pmsg_ids_q1'\n",
      " 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2' 'pmsg_ids_q2'\n",
      " 'pmsg_ids_q2' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1' 'qids_q1'\n",
      " 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'qids_q2' 'q1_cat'\n",
      " 'q1_cat' 'q2_cat' 'q2_cat']\n",
      "0\t train loss: 0.1813 train acc: 56.197% test acc: 53.785%\n",
      "5\t train loss: 0.1637 train acc: 56.289% test acc: 53.984%\n",
      "10\t train loss: 0.1608 train acc: 56.289% test acc: 53.984%\n",
      "15\t train loss: 0.1578 train acc: 57.198% test acc: 54.648%\n",
      "20\t train loss: 0.1546 train acc: 57.537% test acc: 54.781%\n",
      "25\t train loss: 0.1520 train acc: 58.091% test acc: 54.382%\n",
      "30\t train loss: 0.1511 train acc: 58.537% test acc: 54.648%\n",
      "35\t train loss: 0.1494 train acc: 58.953% test acc: 55.046%\n",
      "40\t train loss: 0.1480 train acc: 59.938% test acc: 54.914%\n",
      "45\t train loss: 0.1461 train acc: 60.600% test acc: 55.046%\n",
      "50\t train loss: 0.1448 train acc: 61.339% test acc: 54.714%\n",
      "55\t train loss: 0.1442 train acc: 61.309% test acc: 54.847%\n",
      "60\t train loss: 0.1433 train acc: 61.940% test acc: 54.980%\n",
      "65\t train loss: 0.1426 train acc: 62.109% test acc: 55.511%\n",
      "70\t train loss: 0.1415 train acc: 62.325% test acc: 55.179%\n",
      "75\t train loss: 0.1401 train acc: 62.648% test acc: 55.378%\n",
      "80\t train loss: 0.1392 train acc: 63.079% test acc: 55.312%\n",
      "85\t train loss: 0.1382 train acc: 63.387% test acc: 55.511%\n",
      "90\t train loss: 0.1378 train acc: 63.572% test acc: 54.847%\n",
      "95\t train loss: 0.1415 train acc: 63.264% test acc: 54.250%\n",
      "100\t train loss: 0.1366 train acc: 63.202% test acc: 54.714%\n",
      "105\t train loss: 0.1362 train acc: 63.865% test acc: 55.644%\n",
      "110\t train loss: 0.1357 train acc: 64.142% test acc: 55.644%\n",
      "115\t train loss: 0.1349 train acc: 64.881% test acc: 54.914%\n",
      "120\t train loss: 0.1335 train acc: 65.558% test acc: 53.984%\n",
      "125\t train loss: 0.1329 train acc: 65.789% test acc: 54.980%\n",
      "130\t train loss: 0.1316 train acc: 66.328% test acc: 53.718%\n",
      "135\t train loss: 0.1306 train acc: 66.543% test acc: 53.519%\n",
      "140\t train loss: 0.1299 train acc: 66.790% test acc: 53.453%\n",
      "145\t train loss: 0.1305 train acc: 66.928% test acc: 52.722%\n",
      "150\t train loss: 0.1291 train acc: 67.252% test acc: 53.254%\n",
      "155\t train loss: 0.1301 train acc: 67.791% test acc: 53.453%\n",
      "160\t train loss: 0.1289 train acc: 66.359% test acc: 54.050%\n",
      "165\t train loss: 0.1294 train acc: 67.113% test acc: 52.855%\n",
      "170\t train loss: 0.1287 train acc: 68.253% test acc: 53.187%\n",
      "175\t train loss: 0.1257 train acc: 67.975% test acc: 53.453%\n",
      "180\t train loss: 0.1260 train acc: 68.483% test acc: 53.453%\n",
      "185\t train loss: 0.1244 train acc: 68.961% test acc: 53.519%\n",
      "190\t train loss: 0.1238 train acc: 69.222% test acc: 53.785%\n",
      "195\t train loss: 0.1222 train acc: 69.869% test acc: 54.515%\n",
      "199\t train loss: 0.1224 train acc: 69.530% test acc: 54.117%\n"
     ]
    }
   ],
   "source": [
    "for respond_perc in [.5]:\n",
    "# for respond_perc in [.75, .25]:\n",
    "    # for estate, include_state in [(True, True)]:\n",
    "    # for splitQ, splitM in [(True, False), (False, False)]:\n",
    "    for splitQ, splitM in [(False, True)]:\n",
    "        for estate, include_state, fullq in [(True, True, True)]:\n",
    "            for fulls, insertpreds in [(False, True)]:\n",
    "                for model, learning_rate, epochs in [(\"AdaptableLSTM\", .07, 200),]:\n",
    "                # for model, learning_rate, epochs in [(\"BasicNN\", .0054, 500), (\"LogisticRegressor\", .003, 500)]:\n",
    "                    for smooth, noise in [(0, .07)]:\n",
    "                        for loss_fn in [\"MSELoss\"]:\n",
    "                            test_metrics, train_metrics, adjusted_losses = [], [], []\n",
    "                            for seed in range(3):\n",
    "                                np.random.seed(seed)\n",
    "                                torch.manual_seed(seed)\n",
    "                                e = Experiment(\n",
    "                                    numValFolds = 5,\n",
    "                                    epochsToUpdateLabelMods = 10,\n",
    "                                    data_kw={\"minw\": 2,\n",
    "                                            \"maxw\": 31,\n",
    "                                            \"include_state\": include_state,\n",
    "                                            \"include_pid\": False,\n",
    "                                            \"expanded_states\": estate,\n",
    "                                            \"top_respond_perc\": respond_perc,\n",
    "                                             \"full_questionnaire\": fullq,\n",
    "                                             \"full_sequence\": fulls,\n",
    "                                             \"insert_predictions\": insertpreds,\n",
    "                                             \"split_model_features\": splitM,\n",
    "                                             \"split_weekly_questions\": splitQ\n",
    "                                            },\n",
    "                                    model=model,\n",
    "                                    model_kw={\n",
    "                                        \"lossfn\": loss_fn,\n",
    "                                        # \"lossfn\": \"NDCG\",\n",
    "                                        # \"lossfn\": \"CrossEntropyLoss\",\n",
    "                                        \"hidden_size\": 25, \n",
    "                                        \"lr_step_mult\": .9, \n",
    "                                        \"lr_step_epochs\": 60,\n",
    "                                        \"opt_kw\": {\n",
    "                                            \"lr\": learning_rate\n",
    "                                        },\n",
    "                                        \"labelSmoothPerc\": smooth,\n",
    "                                        \"gaussianNoiseStd\": noise,\n",
    "                                        \"splitModel\": splitM,\n",
    "                                        \"splitWeeklyQuestions\": splitQ\n",
    "                                    },\n",
    "                                    train_kw={\n",
    "                                        \"epochs\": epochs,\n",
    "                                        \"n_subj\": 500,\n",
    "                                        \"rec_every\": 5,\n",
    "                                    })\n",
    "                                # torch.autograd.set_detect_anomaly(True)\n",
    "                                report = e.run()\n",
    "\n",
    "\n",
    "\n",
    "                                individual_test_scores, labels = e.report_scores_individual_test()\n",
    "                                individual_train_scores, labels = e.report_scores_individual_train()\n",
    "\n",
    "\n",
    "\n",
    "                                dire = \"./experiment_output/\"\n",
    "                                fileprefix = f\"{model}LR{learning_rate}Resp{respond_perc}States{int(include_state)}Expanded{int(estate)}Seq{int(fulls)}Pred{int(insertpreds)}Smooth{smooth}Noise{noise}Split{int(splitQ)}{int(splitM)}\"\n",
    "                                np.savetxt(f\"{dire}TRAINMETRICS-{fileprefix}S{seed}.csv\", report[\"train_metrics\"], delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}TESTMETRICS-{fileprefix}S{seed}.csv\", report[\"test_metrics\"], delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}IDVDTESTMETRICS-{fileprefix}S{seed}.csv\", individual_test_scores, delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}IDVDTRAINMETRICS-{fileprefix}S{seed}.csv\", individual_train_scores, delimiter = ',', header = ','.join(report['metric_labels']))\n",
    "                                np.savetxt(f\"{dire}TRAINLOSSES-{fileprefix}S{seed}.csv\", report[\"loss\"], delimiter = ',')\n",
    "\n",
    "                                preds1, preds2, preds3 = e.get_class_predictions(False)\n",
    "\n",
    "\n",
    "                                np.savetxt(f\"{dire}TRAINPREDS1-{fileprefix}S{seed}.csv\", preds1, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TRAINPREDS2-{fileprefix}S{seed}.csv\", preds2, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TRAINPREDS3-{fileprefix}S{seed}.csv\", preds3, delimiter = ',')\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds1)\n",
    "                                plt.title(\"Train Predictions for Class 1\")\n",
    "                                plt.savefig(f\"{dire}/img/C1PredsTrain-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds2)\n",
    "                                plt.title(\"Train Predictions for Class 2\")\n",
    "                                plt.savefig(f\"{dire}/img/C2PredsTrain-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds3)\n",
    "                                plt.title(\"Train Predictions for Class 3\")\n",
    "                                plt.savefig(f\"{dire}/img/C3PredsTrain-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                preds1, preds2, preds3 = e.get_class_predictions(True)\n",
    "\n",
    "                                np.savetxt(f\"{dire}TESTPREDS1-{fileprefix}S{seed}.csv\", preds1, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TESTPREDS2-{fileprefix}S{seed}.csv\", preds2, delimiter = ',')\n",
    "                                np.savetxt(f\"{dire}TESTPREDS3-{fileprefix}S{seed}.csv\", preds3, delimiter = ',')\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds1)\n",
    "                                plt.title(\"Test Predictions for Class 1\")\n",
    "                                plt.savefig(f\"{dire}/img/C1PredsTest-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds2)\n",
    "                                plt.title(\"Test Predictions for Class 2\")\n",
    "                                plt.savefig(f\"{dire}/img/C2PredsTest-{fileprefix}S{seed}.png\")\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.hist(preds3)\n",
    "                                plt.title(\"Test Predictions for Class 3\")\n",
    "                                plt.savefig(f\"{dire}/img/C3PredsTest-{fileprefix}S{seed}.png\")\n",
    "\n",
    "\n",
    "                                plt.clf()\n",
    "                                splot = plt.scatter(individual_test_scores[:, -1], individual_test_scores[:, labels.index(\"Acc\")])\n",
    "                                plt.title(\"Test Accuracy vs Response Count\")\n",
    "                                plt.savefig(f\"{dire}/img/TestACCvResponse-{fileprefix}S{seed}.png\")\n",
    "                                plt.clf()\n",
    "                                splot = plt.scatter(individual_train_scores[:, -1], individual_train_scores[:, labels.index(\"Acc\")])\n",
    "                                plt.title(\"Train Accuracy vs Response Count\")\n",
    "                                plt.savefig(f\"{dire}/img/TrainACCvResponse-{fileprefix}S{seed}.png\")\n",
    "                                plt.clf()\n",
    "\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"Acc\")], label=\"Train Acc.\")\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"Acc\")], label=\"Test Acc.\")\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"MSE\")], label=\"Train MSE\")\n",
    "                                splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"MSE\")], label=\"Test MSE\")\n",
    "                                plt.title(\"Train/Test Performance Over Training\")\n",
    "                                plt.legend()\n",
    "                                plt.ylabel(\"Metric\")\n",
    "                                plt.xlabel(\"Training Epoch\")\n",
    "                                plt.savefig(f\"{dire}/img/AccuracyVsEpoch-{fileprefix}S{seed}.png\")\n",
    "                                plt.clf()\n",
    "\n",
    "\n",
    "                                writer = open(f\"{dire}ALOSS-{fileprefix}.csv\", \"a\")\n",
    "                                writer.write(\",\".join([str(loss[0]) for loss in report[\"loss\"]]))\n",
    "                                writer.write(\"\\n\")\n",
    "                                writer.close()\n",
    "\n",
    "                                writer = open(f\"{dire}FINALTRAINMETRICS-{fileprefix}.csv\", \"a\")\n",
    "                                writer.write(\",\".join([str(loss) for loss in report[\"train_metrics\"][-1, :]]))\n",
    "                                writer.write(\"\\n\")\n",
    "                                writer.close()\n",
    "\n",
    "                                writer = open(f\"{dire}FINALTESTMETRICS-{fileprefix}.csv\", \"a\")\n",
    "                                writer.write(\",\".join([str(loss) for loss in report[\"test_metrics\"][-1, :]]))\n",
    "                                writer.write(\"\\n\")\n",
    "                                writer.close()\n",
    "\n",
    "                                Plotter.training_loss(report, dire)\n",
    "\n",
    "                                plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea7b5b-a82c-4039-b292-11c96933e9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6e339f590beffb2e62e02c6be9b431caf4c76db3ef9baeb9786d6033ee27a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
