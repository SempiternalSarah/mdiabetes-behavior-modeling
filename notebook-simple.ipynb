{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23791e19-fa29-4321-9a92-62c66249e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "from utils.behavior_data import BehaviorData\n",
    "from visuals import Plotter\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.state_data import StateData\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ccab30-3fb9-4990-8492-e7d81a539a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "214\n",
      "0\t train loss: 0.1627 train acc: 42.026% test acc: 40.964%\n",
      "5\t train loss: 0.1543 train acc: 51.854% test acc: 50.488%\n",
      "10\t train loss: 0.1516 train acc: 51.854% test acc: 50.488%\n",
      "15\t train loss: 0.1502 train acc: 51.854% test acc: 50.488%\n",
      "20\t train loss: 0.1487 train acc: 51.886% test acc: 50.545%\n",
      "25\t train loss: 0.1465 train acc: 52.365% test acc: 51.234%\n",
      "30\t train loss: 0.1463 train acc: 53.052% test acc: 51.291%\n",
      "35\t train loss: 0.1428 train acc: 53.531% test acc: 52.037%\n",
      "40\t train loss: 0.1412 train acc: 54.011% test acc: 52.496%\n",
      "45\t train loss: 0.1389 train acc: 54.970% test acc: 53.701%\n",
      "50\t train loss: 0.1383 train acc: 55.673% test acc: 54.332%\n",
      "55\t train loss: 0.1371 train acc: 56.136% test acc: 55.536%\n",
      "60\t train loss: 0.1356 train acc: 56.392% test acc: 55.307%\n",
      "65\t train loss: 0.1342 train acc: 57.127% test acc: 55.307%\n",
      "70\t train loss: 0.1326 train acc: 58.022% test acc: 56.053%\n",
      "75\t train loss: 0.1308 train acc: 57.942% test acc: 55.594%\n",
      "80\t train loss: 0.1305 train acc: 59.028% test acc: 56.397%\n",
      "85\t train loss: 0.1294 train acc: 59.316% test acc: 55.594%\n",
      "90\t train loss: 0.1277 train acc: 59.811% test acc: 55.823%\n",
      "95\t train loss: 0.1262 train acc: 60.531% test acc: 55.479%\n",
      "100\t train loss: 0.1250 train acc: 61.058% test acc: 54.963%\n",
      "105\t train loss: 0.1240 train acc: 61.953% test acc: 54.446%\n",
      "110\t train loss: 0.1230 train acc: 62.400% test acc: 54.791%\n",
      "115\t train loss: 0.1224 train acc: 62.736% test acc: 55.307%\n",
      "120\t train loss: 0.1208 train acc: 63.007% test acc: 54.733%\n",
      "125\t train loss: 0.1205 train acc: 63.487% test acc: 55.364%\n",
      "130\t train loss: 0.1191 train acc: 64.509% test acc: 55.077%\n",
      "135\t train loss: 0.1184 train acc: 64.765% test acc: 54.733%\n",
      "140\t train loss: 0.1177 train acc: 64.813% test acc: 55.307%\n",
      "145\t train loss: 0.1171 train acc: 65.820% test acc: 55.250%\n",
      "150\t train loss: 0.1157 train acc: 65.900% test acc: 55.020%\n",
      "155\t train loss: 0.1150 train acc: 66.075% test acc: 55.077%\n",
      "160\t train loss: 0.1139 train acc: 66.635% test acc: 55.422%\n",
      "165\t train loss: 0.1132 train acc: 66.938% test acc: 55.422%\n",
      "170\t train loss: 0.1137 train acc: 67.082% test acc: 55.766%\n",
      "175\t train loss: 0.1129 train acc: 67.801% test acc: 54.905%\n",
      "180\t train loss: 0.1117 train acc: 67.881% test acc: 55.422%\n",
      "185\t train loss: 0.1111 train acc: 68.073% test acc: 55.479%\n",
      "190\t train loss: 0.1102 train acc: 68.041% test acc: 55.364%\n",
      "195\t train loss: 0.1103 train acc: 68.536% test acc: 55.077%\n",
      "200\t train loss: 0.1092 train acc: 69.143% test acc: 55.422%\n",
      "205\t train loss: 0.1096 train acc: 69.223% test acc: 55.077%\n",
      "210\t train loss: 0.1084 train acc: 69.639% test acc: 54.045%\n",
      "215\t train loss: 0.1068 train acc: 69.990% test acc: 54.274%\n",
      "220\t train loss: 0.1067 train acc: 69.751% test acc: 53.586%\n",
      "225\t train loss: 0.1071 train acc: 70.598% test acc: 53.643%\n",
      "230\t train loss: 0.1056 train acc: 70.662% test acc: 54.217%\n",
      "235\t train loss: 0.1050 train acc: 70.725% test acc: 53.299%\n",
      "240\t train loss: 0.1032 train acc: 71.269% test acc: 53.528%\n",
      "245\t train loss: 0.1041 train acc: 71.429% test acc: 53.012%\n",
      "250\t train loss: 0.1027 train acc: 71.365% test acc: 53.069%\n",
      "255\t train loss: 0.1020 train acc: 71.556% test acc: 52.840%\n",
      "260\t train loss: 0.1029 train acc: 71.876% test acc: 52.496%\n",
      "265\t train loss: 0.1013 train acc: 72.132% test acc: 52.553%\n",
      "270\t train loss: 0.1003 train acc: 72.307% test acc: 52.610%\n",
      "275\t train loss: 0.1016 train acc: 72.723% test acc: 52.553%\n",
      "280\t train loss: 0.1003 train acc: 72.883% test acc: 52.553%\n",
      "285\t train loss: 0.1000 train acc: 73.170% test acc: 52.955%\n",
      "290\t train loss: 0.0996 train acc: 73.138% test acc: 52.553%\n",
      "295\t train loss: 0.0985 train acc: 73.170% test acc: 52.668%\n",
      "300\t train loss: 0.0981 train acc: 73.650% test acc: 52.381%\n",
      "305\t train loss: 0.0979 train acc: 73.905% test acc: 52.094%\n",
      "310\t train loss: 0.0970 train acc: 73.746% test acc: 52.094%\n",
      "315\t train loss: 0.0968 train acc: 74.241% test acc: 52.266%\n",
      "320\t train loss: 0.0965 train acc: 74.065% test acc: 51.692%\n",
      "325\t train loss: 0.0960 train acc: 74.433% test acc: 51.463%\n",
      "330\t train loss: 0.0964 train acc: 74.513% test acc: 51.463%\n",
      "335\t train loss: 0.0956 train acc: 74.688% test acc: 51.119%\n",
      "340\t train loss: 0.0953 train acc: 74.369% test acc: 51.004%\n",
      "345\t train loss: 0.0950 train acc: 74.704% test acc: 51.291%\n",
      "350\t train loss: 0.0938 train acc: 74.640% test acc: 51.119%\n",
      "355\t train loss: 0.0943 train acc: 75.040% test acc: 50.889%\n",
      "360\t train loss: 0.0935 train acc: 75.184% test acc: 51.119%\n",
      "365\t train loss: 0.0937 train acc: 75.455% test acc: 51.348%\n",
      "370\t train loss: 0.0930 train acc: 75.759% test acc: 50.832%\n",
      "375\t train loss: 0.0931 train acc: 75.919% test acc: 51.234%\n",
      "380\t train loss: 0.0916 train acc: 75.919% test acc: 50.373%\n",
      "385\t train loss: 0.0926 train acc: 75.919% test acc: 50.201%\n",
      "390\t train loss: 0.0929 train acc: 76.047% test acc: 50.660%\n",
      "395\t train loss: 0.0914 train acc: 76.382% test acc: 50.373%\n",
      "400\t train loss: 0.0922 train acc: 76.446% test acc: 50.660%\n",
      "405\t train loss: 0.0901 train acc: 76.686% test acc: 50.545%\n",
      "410\t train loss: 0.0914 train acc: 76.398% test acc: 50.602%\n",
      "415\t train loss: 0.0907 train acc: 76.846% test acc: 50.488%\n",
      "420\t train loss: 0.0901 train acc: 76.973% test acc: 50.029%\n",
      "425\t train loss: 0.0900 train acc: 77.133% test acc: 50.201%\n",
      "430\t train loss: 0.0897 train acc: 77.021% test acc: 50.258%\n",
      "435\t train loss: 0.0896 train acc: 77.325% test acc: 49.857%\n",
      "440\t train loss: 0.0898 train acc: 77.165% test acc: 50.201%\n",
      "445\t train loss: 0.0891 train acc: 77.421% test acc: 50.488%\n",
      "450\t train loss: 0.0895 train acc: 77.277% test acc: 49.971%\n",
      "455\t train loss: 0.0880 train acc: 77.613% test acc: 50.602%\n",
      "460\t train loss: 0.0891 train acc: 77.533% test acc: 49.627%\n",
      "465\t train loss: 0.0887 train acc: 77.677% test acc: 49.570%\n",
      "470\t train loss: 0.0885 train acc: 77.357% test acc: 49.914%\n",
      "475\t train loss: 0.0888 train acc: 77.341% test acc: 49.742%\n",
      "480\t train loss: 0.0872 train acc: 77.517% test acc: 49.742%\n",
      "485\t train loss: 0.0878 train acc: 77.645% test acc: 49.857%\n",
      "490\t train loss: 0.0874 train acc: 77.900% test acc: 49.742%\n",
      "495\t train loss: 0.0870 train acc: 78.188% test acc: 49.053%\n",
      "500\t train loss: 0.0874 train acc: 78.140% test acc: 49.340%\n",
      "505\t train loss: 0.0873 train acc: 78.444% test acc: 48.709%\n",
      "510\t train loss: 0.0865 train acc: 78.348% test acc: 48.480%\n",
      "515\t train loss: 0.0863 train acc: 78.380% test acc: 48.021%\n",
      "520\t train loss: 0.0863 train acc: 78.348% test acc: 48.480%\n",
      "525\t train loss: 0.0873 train acc: 78.539% test acc: 48.537%\n",
      "530\t train loss: 0.0865 train acc: 78.603% test acc: 48.422%\n",
      "535\t train loss: 0.0866 train acc: 78.699% test acc: 48.250%\n",
      "540\t train loss: 0.0861 train acc: 78.348% test acc: 48.594%\n",
      "545\t train loss: 0.0858 train acc: 78.619% test acc: 48.078%\n",
      "550\t train loss: 0.0871 train acc: 78.108% test acc: 48.766%\n",
      "555\t train loss: 0.0873 train acc: 78.428% test acc: 48.537%\n",
      "560\t train loss: 0.0862 train acc: 78.156% test acc: 47.562%\n",
      "565\t train loss: 0.0857 train acc: 78.348% test acc: 48.193%\n",
      "570\t train loss: 0.0864 train acc: 78.795% test acc: 48.709%\n",
      "575\t train loss: 0.0850 train acc: 78.779% test acc: 48.766%\n",
      "580\t train loss: 0.0842 train acc: 78.779% test acc: 48.365%\n",
      "585\t train loss: 0.0855 train acc: 79.131% test acc: 48.365%\n",
      "590\t train loss: 0.0838 train acc: 78.955% test acc: 48.135%\n",
      "595\t train loss: 0.0853 train acc: 78.955% test acc: 48.193%\n",
      "600\t train loss: 0.0849 train acc: 79.131% test acc: 48.594%\n",
      "605\t train loss: 0.0845 train acc: 79.418% test acc: 48.594%\n",
      "610\t train loss: 0.0841 train acc: 79.530% test acc: 48.709%\n",
      "615\t train loss: 0.0835 train acc: 79.690% test acc: 48.537%\n",
      "620\t train loss: 0.0837 train acc: 79.594% test acc: 48.996%\n",
      "625\t train loss: 0.0835 train acc: 79.722% test acc: 48.021%\n",
      "630\t train loss: 0.0833 train acc: 79.275% test acc: 47.791%\n",
      "635\t train loss: 0.0846 train acc: 79.818% test acc: 48.652%\n",
      "640\t train loss: 0.0835 train acc: 79.882% test acc: 48.996%\n",
      "645\t train loss: 0.0840 train acc: 79.962% test acc: 48.766%\n",
      "650\t train loss: 0.0830 train acc: 80.010% test acc: 48.135%\n",
      "655\t train loss: 0.0832 train acc: 80.265% test acc: 47.734%\n",
      "660\t train loss: 0.0826 train acc: 79.994% test acc: 47.734%\n",
      "665\t train loss: 0.0826 train acc: 79.898% test acc: 48.250%\n",
      "670\t train loss: 0.0829 train acc: 80.121% test acc: 48.250%\n",
      "675\t train loss: 0.0825 train acc: 80.217% test acc: 48.078%\n",
      "680\t train loss: 0.0833 train acc: 80.074% test acc: 48.193%\n",
      "685\t train loss: 0.0824 train acc: 79.978% test acc: 48.193%\n",
      "690\t train loss: 0.0813 train acc: 79.930% test acc: 48.021%\n",
      "695\t train loss: 0.0822 train acc: 79.706% test acc: 47.906%\n",
      "700\t train loss: 0.0818 train acc: 80.137% test acc: 47.963%\n",
      "705\t train loss: 0.0826 train acc: 80.201% test acc: 47.849%\n",
      "710\t train loss: 0.0817 train acc: 80.457% test acc: 48.250%\n",
      "715\t train loss: 0.0819 train acc: 80.521% test acc: 48.308%\n",
      "720\t train loss: 0.0815 train acc: 80.169% test acc: 48.250%\n",
      "725\t train loss: 0.0822 train acc: 80.489% test acc: 48.480%\n",
      "730\t train loss: 0.0823 train acc: 80.281% test acc: 48.250%\n",
      "735\t train loss: 0.0808 train acc: 80.457% test acc: 47.619%\n",
      "740\t train loss: 0.0817 train acc: 80.521% test acc: 47.906%\n",
      "745\t train loss: 0.0808 train acc: 80.649% test acc: 47.562%\n",
      "750\t train loss: 0.0806 train acc: 80.569% test acc: 47.963%\n",
      "755\t train loss: 0.0816 train acc: 80.841% test acc: 48.021%\n",
      "760\t train loss: 0.0808 train acc: 80.601% test acc: 48.365%\n",
      "765\t train loss: 0.0810 train acc: 80.745% test acc: 48.537%\n",
      "770\t train loss: 0.0807 train acc: 80.697% test acc: 48.250%\n",
      "775\t train loss: 0.0811 train acc: 80.809% test acc: 48.480%\n",
      "780\t train loss: 0.0811 train acc: 80.569% test acc: 48.193%\n",
      "785\t train loss: 0.0813 train acc: 80.825% test acc: 48.480%\n",
      "790\t train loss: 0.0800 train acc: 80.904% test acc: 48.365%\n",
      "795\t train loss: 0.0799 train acc: 80.888% test acc: 48.308%\n",
      "800\t train loss: 0.0802 train acc: 80.872% test acc: 48.193%\n",
      "805\t train loss: 0.0803 train acc: 80.729% test acc: 48.537%\n",
      "810\t train loss: 0.0805 train acc: 80.697% test acc: 48.250%\n",
      "815\t train loss: 0.0802 train acc: 81.096% test acc: 48.480%\n",
      "820\t train loss: 0.0801 train acc: 80.825% test acc: 48.135%\n",
      "825\t train loss: 0.0798 train acc: 81.128% test acc: 48.250%\n",
      "830\t train loss: 0.0794 train acc: 80.585% test acc: 48.021%\n",
      "835\t train loss: 0.0807 train acc: 80.777% test acc: 47.963%\n",
      "840\t train loss: 0.0796 train acc: 80.841% test acc: 48.824%\n",
      "845\t train loss: 0.0804 train acc: 81.160% test acc: 48.480%\n",
      "850\t train loss: 0.0796 train acc: 80.825% test acc: 48.021%\n",
      "855\t train loss: 0.0803 train acc: 81.176% test acc: 48.193%\n",
      "860\t train loss: 0.0789 train acc: 81.208% test acc: 47.906%\n",
      "865\t train loss: 0.0800 train acc: 81.176% test acc: 47.504%\n",
      "870\t train loss: 0.0791 train acc: 81.224% test acc: 48.193%\n",
      "875\t train loss: 0.0801 train acc: 81.176% test acc: 48.250%\n",
      "880\t train loss: 0.0794 train acc: 81.288% test acc: 47.906%\n",
      "885\t train loss: 0.0795 train acc: 81.256% test acc: 48.135%\n",
      "890\t train loss: 0.0785 train acc: 81.112% test acc: 47.791%\n",
      "895\t train loss: 0.0796 train acc: 81.160% test acc: 47.849%\n",
      "900\t train loss: 0.0788 train acc: 81.256% test acc: 47.734%\n",
      "905\t train loss: 0.0784 train acc: 81.480% test acc: 47.791%\n",
      "910\t train loss: 0.0796 train acc: 81.432% test acc: 48.193%\n",
      "915\t train loss: 0.0787 train acc: 81.624% test acc: 48.193%\n",
      "920\t train loss: 0.0777 train acc: 81.160% test acc: 48.250%\n",
      "925\t train loss: 0.0787 train acc: 81.240% test acc: 48.135%\n",
      "930\t train loss: 0.0778 train acc: 81.240% test acc: 48.021%\n",
      "935\t train loss: 0.0788 train acc: 81.608% test acc: 48.135%\n",
      "940\t train loss: 0.0785 train acc: 81.288% test acc: 48.021%\n",
      "945\t train loss: 0.0783 train acc: 81.512% test acc: 48.021%\n",
      "950\t train loss: 0.0782 train acc: 81.512% test acc: 48.422%\n",
      "955\t train loss: 0.0781 train acc: 81.464% test acc: 48.709%\n",
      "960\t train loss: 0.0775 train acc: 81.719% test acc: 48.365%\n",
      "965\t train loss: 0.0788 train acc: 81.624% test acc: 48.193%\n",
      "970\t train loss: 0.0780 train acc: 80.857% test acc: 48.365%\n",
      "975\t train loss: 0.0783 train acc: 81.256% test acc: 48.135%\n",
      "980\t train loss: 0.0783 train acc: 81.304% test acc: 48.652%\n",
      "985\t train loss: 0.0787 train acc: 81.560% test acc: 48.480%\n",
      "990\t train loss: 0.0775 train acc: 81.272% test acc: 48.250%\n",
      "995\t train loss: 0.0786 train acc: 81.624% test acc: 48.766%\n",
      "999\t train loss: 0.0785 train acc: 81.384% test acc: 48.594%\n"
     ]
    }
   ],
   "source": [
    "model, learning_rate = \"AdaptableLSTM\", .01\n",
    "# model, learning_rate = \"BasicNN\", .0054\n",
    "# model, learning_rate = \"LogisticRegressor\", .003\n",
    "epochs = 1000\n",
    "seed = 1\n",
    "include_state = True\n",
    "estate = True\n",
    "fullq = True\n",
    "respond_perc = .50\n",
    "fullseq = False\n",
    "insertpreds = True\n",
    "noise = 0.15\n",
    "smooth = 0\n",
    "\n",
    "    \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "e = Experiment(\n",
    "    numValFolds = 5,\n",
    "    epochsToUpdateLabelMods = 10,\n",
    "    data_kw={\"minw\": 2,\n",
    "            \"maxw\": 31,\n",
    "            \"include_state\": include_state,\n",
    "            \"include_pid\": False,\n",
    "            \"expanded_states\": estate,\n",
    "            \"top_respond_perc\": respond_perc,\n",
    "             \"full_questionnaire\": fullq,\n",
    "             \"full_sequence\": fullseq,\n",
    "             \"insert_predictions\": insertpreds,\n",
    "             \"one_hot_response_features\": False,\n",
    "             \"response_feature_noise\": noise,\n",
    "             \"max_state_week\": 1\n",
    "            },\n",
    "    model=model,\n",
    "    model_kw={\n",
    "        \"lossfn\": \"MSELoss\",\n",
    "        # \"lossfn\": \"NDCG\",\n",
    "        # \"lossfn\": \"CrossEntropyLoss\",\n",
    "        \"hidden_size\": 10,\n",
    "        \"lr_step_mult\": .9, \n",
    "        \"lr_step_epochs\": 100,\n",
    "        \"opt_kw\": {\n",
    "            \"lr\": learning_rate,\n",
    "        },\n",
    "        \"labelSmoothPerc\": smooth,\n",
    "        \"gaussianNoiseStd\": noise\n",
    "        \n",
    "    },\n",
    "    train_kw={\n",
    "        \"epochs\": epochs,\n",
    "        \"n_subj\": 500,\n",
    "        \"rec_every\": 5,\n",
    "    })\n",
    "\n",
    "\n",
    "print(len(e.bd.test))\n",
    "print(len(e.bd.train))\n",
    "\n",
    "report = e.run()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71de6743-690e-463b-b5b0-212eb0da9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.23904811e-01 8.74385459e-01 8.93889652e-01 8.57241952e-01\n",
      " 7.45464909e-01 7.45464909e-01 7.45464909e-01 5.24784421e-01\n",
      " 6.70063378e-01 8.86301926e-01 6.57121896e-01 6.82089056e-01\n",
      " 7.91016754e-01 5.24784421e-01 6.70063378e-01 8.86301926e-01\n",
      " 1.58200000e+03 1.43100000e+03 3.24500000e+03 7.38281769e-01\n",
      " 7.48232019e-01 7.47164560e-01 7.84201602e-01 7.57000441e-01\n",
      " 7.41388837e-01 7.95448048e-01 7.55208627e-01 7.64529844e-01\n",
      " 7.68590000e-01 7.49242294e-01 7.24576349e-01 7.40149823e-01\n",
      " 7.36318406e-01 7.65635426e-01 7.11538101e-01 7.46497398e-01\n",
      " 7.50633797e-01 7.28319942e-01 7.23497896e-01 7.39647915e-01\n",
      " 7.40489862e-01 7.33371603e-01 7.00784539e-01]\n",
      "0.8138383030891418\n",
      "0.4859437644481659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print (np.mean(report['train_metrics'], axis=0))\n",
    "labels = report[\"metric_labels\"]\n",
    "print(report['train_metrics'][-1, labels.index(\"Acc\")])\n",
    "print(report['test_metrics'][-1, labels.index(\"Acc\")])\n",
    "\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"Acc\")], label=\"Train Acc.\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"Acc\")], label=\"Test Acc.\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"MSE\")], label=\"Train MSE\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"MSE\")], label=\"Test MSE\")\n",
    "plt.title(\"Train/Test Performance Over Training\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.savefig(\"simpleNotebookAccPlot.png\")\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5928e0-08ac-4055-9bc6-89fe1fea6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6456, 100]) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(e.bd.features.shape, e.bd.featureList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41511c-c598-4167-9570-c6b6a75c927a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6e339f590beffb2e62e02c6be9b431caf4c76db3ef9baeb9786d6033ee27a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
