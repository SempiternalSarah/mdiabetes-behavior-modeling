{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23791e19-fa29-4321-9a92-62c66249e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "from utils.behavior_data import BehaviorData\n",
    "from visuals import Plotter\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.state_data import StateData\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ccab30-3fb9-4990-8492-e7d81a539a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'response_last_1_q1' 'response_last_1_q1'\n",
      " 'response_last_1_q1' 'response_last_1_q2' 'response_last_1_q2'\n",
      " 'response_last_1_q2' 'pmsg_sids_last_0_q1' 'pmsg_sids_last_0_q1'\n",
      " 'pmsg_sids_last_0_q1' 'pmsg_sids_last_0_q1' 'pmsg_sids_last_0_q1'\n",
      " 'pmsg_sids_last_0_q2' 'pmsg_sids_last_0_q2' 'pmsg_sids_last_0_q2'\n",
      " 'pmsg_sids_last_0_q2' 'pmsg_sids_last_0_q2' 'paction_sids_last_0_q1'\n",
      " 'paction_sids_last_0_q1' 'paction_sids_last_0_q1'\n",
      " 'paction_sids_last_0_q1' 'paction_sids_last_0_q1'\n",
      " 'paction_sids_last_0_q2' 'paction_sids_last_0_q2'\n",
      " 'paction_sids_last_0_q2' 'paction_sids_last_0_q2'\n",
      " 'paction_sids_last_0_q2' 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q1'\n",
      " 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q1'\n",
      " 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q2' 'pmsg_ids_last_0_q2'\n",
      " 'pmsg_ids_last_0_q2' 'pmsg_ids_last_0_q2' 'pmsg_ids_last_0_q2'\n",
      " 'pmsg_ids_last_0_q2' 'qids_last_0_q1' 'qids_last_0_q1' 'qids_last_0_q1'\n",
      " 'qids_last_0_q1' 'qids_last_0_q1' 'qids_last_0_q1' 'qids_last_0_q2'\n",
      " 'qids_last_0_q2' 'qids_last_0_q2' 'qids_last_0_q2' 'qids_last_0_q2'\n",
      " 'qids_last_0_q2' 'pmsg_sids_last_1_q1' 'pmsg_sids_last_1_q1'\n",
      " 'pmsg_sids_last_1_q1' 'pmsg_sids_last_1_q1' 'pmsg_sids_last_1_q1'\n",
      " 'pmsg_sids_last_1_q2' 'pmsg_sids_last_1_q2' 'pmsg_sids_last_1_q2'\n",
      " 'pmsg_sids_last_1_q2' 'pmsg_sids_last_1_q2' 'paction_sids_last_1_q1'\n",
      " 'paction_sids_last_1_q1' 'paction_sids_last_1_q1'\n",
      " 'paction_sids_last_1_q1' 'paction_sids_last_1_q1'\n",
      " 'paction_sids_last_1_q2' 'paction_sids_last_1_q2'\n",
      " 'paction_sids_last_1_q2' 'paction_sids_last_1_q2'\n",
      " 'paction_sids_last_1_q2' 'pmsg_ids_last_1_q1' 'pmsg_ids_last_1_q1'\n",
      " 'pmsg_ids_last_1_q1' 'pmsg_ids_last_1_q1' 'pmsg_ids_last_1_q1'\n",
      " 'pmsg_ids_last_1_q1' 'pmsg_ids_last_1_q2' 'pmsg_ids_last_1_q2'\n",
      " 'pmsg_ids_last_1_q2' 'pmsg_ids_last_1_q2' 'pmsg_ids_last_1_q2'\n",
      " 'pmsg_ids_last_1_q2' 'qids_last_1_q1' 'qids_last_1_q1' 'qids_last_1_q1'\n",
      " 'qids_last_1_q1' 'qids_last_1_q1' 'qids_last_1_q1' 'qids_last_1_q2'\n",
      " 'qids_last_1_q2' 'qids_last_1_q2' 'qids_last_1_q2' 'qids_last_1_q2'\n",
      " 'qids_last_1_q2' 'q1_cat' 'q1_cat' 'q2_cat' 'q2_cat']\n",
      "54\n",
      "214\n",
      "0\t train loss: 0.2155 train acc: 54.627% test acc: 52.125% train exerAcc: 46.888% test exerAcc: 48.077% train conAcc: 40.810% test conAcc: 43.485% train knowAcc: 65.945% test knowAcc: 58.690%\n",
      "5\t train loss: 0.1873 train acc: 55.797% test acc: 53.187% train exerAcc: 49.793% test exerAcc: 48.077% train conAcc: 43.363% test conAcc: 46.091% train knowAcc: 65.917% test knowAcc: 58.690%\n",
      "10\t train loss: 0.1843 train acc: 57.998% test acc: 53.984% train exerAcc: 57.676% test exerAcc: 46.154% train conAcc: 47.848% test conAcc: 48.208% train knowAcc: 65.945% test knowAcc: 58.690%\n",
      "15\t train loss: 0.1803 train acc: 58.430% test acc: 54.183% train exerAcc: 62.656% test exerAcc: 48.077% train conAcc: 48.432% test conAcc: 48.534% train knowAcc: 65.945% test knowAcc: 58.690%\n",
      "20\t train loss: 0.1772 train acc: 59.292% test acc: 54.582% train exerAcc: 71.369% test exerAcc: 46.154% train conAcc: 49.708% test conAcc: 49.674% train knowAcc: 65.945% test knowAcc: 58.690%\n",
      "25\t train loss: 0.1741 train acc: 59.738% test acc: 55.113% train exerAcc: 72.614% test exerAcc: 51.923% train conAcc: 50.474% test conAcc: 50.977% train knowAcc: 66.088% test knowAcc: 58.333%\n",
      "30\t train loss: 0.1712 train acc: 60.677% test acc: 55.113% train exerAcc: 75.934% test exerAcc: 50.000% train conAcc: 52.371% test conAcc: 51.140% train knowAcc: 66.116% test knowAcc: 58.333%\n",
      "35\t train loss: 0.1689 train acc: 61.186% test acc: 53.851% train exerAcc: 77.593% test exerAcc: 50.000% train conAcc: 52.589% test conAcc: 49.186% train knowAcc: 66.771% test knowAcc: 57.500%\n",
      "40\t train loss: 0.1665 train acc: 61.878% test acc: 53.984% train exerAcc: 80.498% test exerAcc: 48.077% train conAcc: 53.866% test conAcc: 49.511% train knowAcc: 66.856% test knowAcc: 57.619%\n",
      "45\t train loss: 0.1644 train acc: 62.756% test acc: 54.183% train exerAcc: 82.573% test exerAcc: 46.154% train conAcc: 55.288% test conAcc: 50.489% train knowAcc: 67.227% test knowAcc: 57.381%\n",
      "50\t train loss: 0.1624 train acc: 63.187% test acc: 54.382% train exerAcc: 86.307% test exerAcc: 44.231% train conAcc: 55.434% test conAcc: 51.629% train knowAcc: 67.654% test knowAcc: 57.024%\n",
      "55\t train loss: 0.1605 train acc: 63.788% test acc: 54.183% train exerAcc: 87.967% test exerAcc: 46.154% train conAcc: 56.747% test conAcc: 50.814% train knowAcc: 67.625% test knowAcc: 57.143%\n",
      "60\t train loss: 0.1586 train acc: 64.465% test acc: 53.918% train exerAcc: 89.627% test exerAcc: 44.231% train conAcc: 58.133% test conAcc: 50.000% train knowAcc: 67.682% test knowAcc: 57.381%\n",
      "65\t train loss: 0.1566 train acc: 65.158% test acc: 53.785% train exerAcc: 91.701% test exerAcc: 42.308% train conAcc: 59.263% test conAcc: 50.326% train knowAcc: 67.938% test knowAcc: 57.024%\n",
      "70\t train loss: 0.1549 train acc: 65.650% test acc: 53.718% train exerAcc: 92.946% test exerAcc: 42.308% train conAcc: 59.956% test conAcc: 49.837% train knowAcc: 68.223% test knowAcc: 57.262%\n",
      "75\t train loss: 0.1534 train acc: 66.451% test acc: 53.984% train exerAcc: 93.361% test exerAcc: 42.308% train conAcc: 61.415% test conAcc: 50.326% train knowAcc: 68.536% test knowAcc: 57.381%\n",
      "80\t train loss: 0.1516 train acc: 66.605% test acc: 53.519% train exerAcc: 94.191% test exerAcc: 42.308% train conAcc: 61.196% test conAcc: 49.674% train knowAcc: 68.935% test knowAcc: 57.024%\n",
      "85\t train loss: 0.1501 train acc: 67.313% test acc: 53.519% train exerAcc: 95.021% test exerAcc: 44.231% train conAcc: 62.254% test conAcc: 49.349% train knowAcc: 69.362% test knowAcc: 57.143%\n",
      "90\t train loss: 0.1484 train acc: 67.698% test acc: 53.453% train exerAcc: 95.021% test exerAcc: 44.231% train conAcc: 62.947% test conAcc: 48.697% train knowAcc: 69.533% test knowAcc: 57.500%\n",
      "95\t train loss: 0.1469 train acc: 68.283% test acc: 53.054% train exerAcc: 95.021% test exerAcc: 42.308% train conAcc: 63.713% test conAcc: 48.534% train knowAcc: 70.017% test knowAcc: 57.024%\n",
      "100\t train loss: 0.1451 train acc: 68.714% test acc: 53.320% train exerAcc: 95.436% test exerAcc: 42.308% train conAcc: 64.624% test conAcc: 48.860% train knowAcc: 70.074% test knowAcc: 57.262%\n",
      "105\t train loss: 0.1438 train acc: 69.145% test acc: 53.785% train exerAcc: 95.851% test exerAcc: 42.308% train conAcc: 65.463% test conAcc: 49.837% train knowAcc: 70.188% test knowAcc: 57.381%\n",
      "110\t train loss: 0.1423 train acc: 69.731% test acc: 53.586% train exerAcc: 96.266% test exerAcc: 40.385% train conAcc: 66.448% test conAcc: 49.349% train knowAcc: 70.473% test knowAcc: 57.500%\n",
      "115\t train loss: 0.1408 train acc: 69.977% test acc: 53.254% train exerAcc: 96.680% test exerAcc: 40.385% train conAcc: 66.703% test conAcc: 50.000% train knowAcc: 70.700% test knowAcc: 56.429%\n",
      "120\t train loss: 0.1395 train acc: 70.408% test acc: 53.320% train exerAcc: 96.680% test exerAcc: 40.385% train conAcc: 67.287% test conAcc: 50.000% train knowAcc: 71.042% test knowAcc: 56.548%\n",
      "125\t train loss: 0.1379 train acc: 70.670% test acc: 53.652% train exerAcc: 96.680% test exerAcc: 40.385% train conAcc: 67.396% test conAcc: 50.163% train knowAcc: 71.441% test knowAcc: 57.024%\n",
      "130\t train loss: 0.1364 train acc: 71.255% test acc: 53.187% train exerAcc: 96.680% test exerAcc: 40.385% train conAcc: 68.527% test conAcc: 49.511% train knowAcc: 71.640% test knowAcc: 56.667%\n",
      "135\t train loss: 0.1355 train acc: 71.917% test acc: 53.453% train exerAcc: 96.680% test exerAcc: 40.385% train conAcc: 69.694% test conAcc: 49.349% train knowAcc: 71.953% test knowAcc: 57.262%\n",
      "140\t train loss: 0.1342 train acc: 72.317% test acc: 52.789% train exerAcc: 96.680% test exerAcc: 40.385% train conAcc: 70.204% test conAcc: 48.534% train knowAcc: 72.295% test knowAcc: 56.667%\n",
      "145\t train loss: 0.1327 train acc: 72.440% test acc: 53.320% train exerAcc: 96.680% test exerAcc: 42.308% train conAcc: 70.314% test conAcc: 49.349% train knowAcc: 72.437% test knowAcc: 56.905%\n",
      "150\t train loss: 0.1312 train acc: 72.702% test acc: 53.121% train exerAcc: 96.680% test exerAcc: 42.308% train conAcc: 70.934% test conAcc: 49.023% train knowAcc: 72.437% test knowAcc: 56.786%\n",
      "155\t train loss: 0.1303 train acc: 72.902% test acc: 53.320% train exerAcc: 96.680% test exerAcc: 42.308% train conAcc: 70.605% test conAcc: 49.349% train knowAcc: 73.064% test knowAcc: 56.905%\n",
      "160\t train loss: 0.1285 train acc: 73.595% test acc: 52.722% train exerAcc: 97.095% test exerAcc: 42.308% train conAcc: 71.517% test conAcc: 49.349% train knowAcc: 73.605% test knowAcc: 55.833%\n",
      "165\t train loss: 0.1282 train acc: 73.995% test acc: 52.191% train exerAcc: 97.095% test exerAcc: 42.308% train conAcc: 72.101% test conAcc: 48.534% train knowAcc: 73.890% test knowAcc: 55.476%\n",
      "170\t train loss: 0.1268 train acc: 74.011% test acc: 52.058% train exerAcc: 97.095% test exerAcc: 42.308% train conAcc: 71.736% test conAcc: 47.394% train knowAcc: 74.203% test knowAcc: 56.071%\n",
      "175\t train loss: 0.1252 train acc: 74.642% test acc: 51.926% train exerAcc: 97.095% test exerAcc: 40.385% train conAcc: 72.648% test conAcc: 47.557% train knowAcc: 74.658% test knowAcc: 55.833%\n",
      "180\t train loss: 0.1239 train acc: 75.196% test acc: 52.457% train exerAcc: 97.095% test exerAcc: 38.462% train conAcc: 73.450% test conAcc: 48.860% train knowAcc: 75.057% test knowAcc: 55.952%\n",
      "185\t train loss: 0.1227 train acc: 75.258% test acc: 52.656% train exerAcc: 97.510% test exerAcc: 40.385% train conAcc: 73.523% test conAcc: 49.023% train knowAcc: 75.085% test knowAcc: 56.071%\n",
      "190\t train loss: 0.1216 train acc: 75.581% test acc: 51.859% train exerAcc: 97.510% test exerAcc: 40.385% train conAcc: 73.924% test conAcc: 48.860% train knowAcc: 75.370% test knowAcc: 54.762%\n",
      "195\t train loss: 0.1214 train acc: 75.982% test acc: 51.394% train exerAcc: 97.510% test exerAcc: 40.385% train conAcc: 74.179% test conAcc: 46.743% train knowAcc: 75.911% test knowAcc: 55.476%\n",
      "200\t train loss: 0.1200 train acc: 75.920% test acc: 51.461% train exerAcc: 97.510% test exerAcc: 42.308% train conAcc: 73.851% test conAcc: 46.743% train knowAcc: 76.054% test knowAcc: 55.476%\n",
      "205\t train loss: 0.1181 train acc: 76.366% test acc: 51.594% train exerAcc: 97.510% test exerAcc: 42.308% train conAcc: 74.617% test conAcc: 46.743% train knowAcc: 76.281% test knowAcc: 55.714%\n",
      "210\t train loss: 0.1171 train acc: 76.597% test acc: 52.789% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 75.310% test conAcc: 48.208% train knowAcc: 76.167% test knowAcc: 56.667%\n",
      "215\t train loss: 0.1164 train acc: 76.813% test acc: 52.590% train exerAcc: 97.510% test exerAcc: 42.308% train conAcc: 75.638% test conAcc: 47.720% train knowAcc: 76.310% test knowAcc: 56.786%\n",
      "220\t train loss: 0.1159 train acc: 77.567% test acc: 51.660% train exerAcc: 97.510% test exerAcc: 42.308% train conAcc: 75.784% test conAcc: 47.720% train knowAcc: 77.591% test knowAcc: 55.119%\n",
      "225\t train loss: 0.1150 train acc: 77.644% test acc: 51.461% train exerAcc: 97.510% test exerAcc: 42.308% train conAcc: 76.003% test conAcc: 47.720% train knowAcc: 77.563% test knowAcc: 54.762%\n",
      "230\t train loss: 0.1132 train acc: 77.906% test acc: 51.726% train exerAcc: 97.510% test exerAcc: 42.308% train conAcc: 76.076% test conAcc: 47.883% train knowAcc: 77.990% test knowAcc: 55.119%\n",
      "235\t train loss: 0.1130 train acc: 78.014% test acc: 51.992% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 76.258% test conAcc: 47.883% train knowAcc: 78.047% test knowAcc: 55.476%\n",
      "240\t train loss: 0.1116 train acc: 77.875% test acc: 52.191% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 75.930% test conAcc: 47.883% train knowAcc: 78.047% test knowAcc: 55.833%\n",
      "245\t train loss: 0.1103 train acc: 78.045% test acc: 52.390% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 76.951% test conAcc: 47.394% train knowAcc: 77.563% test knowAcc: 56.548%\n",
      "250\t train loss: 0.1095 train acc: 78.968% test acc: 52.125% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 77.535% test conAcc: 47.883% train knowAcc: 78.815% test knowAcc: 55.714%\n",
      "255\t train loss: 0.1088 train acc: 79.092% test acc: 52.058% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 77.425% test conAcc: 47.557% train knowAcc: 79.129% test knowAcc: 55.833%\n",
      "260\t train loss: 0.1084 train acc: 78.507% test acc: 51.992% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 77.790% test conAcc: 47.231% train knowAcc: 77.762% test knowAcc: 55.952%\n",
      "265\t train loss: 0.1072 train acc: 78.845% test acc: 52.523% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 78.228% test conAcc: 47.720% train knowAcc: 78.047% test knowAcc: 56.548%\n",
      "270\t train loss: 0.1056 train acc: 79.677% test acc: 52.258% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 78.738% test conAcc: 47.883% train knowAcc: 79.186% test knowAcc: 55.952%\n",
      "275\t train loss: 0.1059 train acc: 79.384% test acc: 50.863% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 77.243% test conAcc: 45.603% train knowAcc: 79.812% test knowAcc: 55.119%\n",
      "280\t train loss: 0.1045 train acc: 80.092% test acc: 51.195% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 78.920% test conAcc: 47.394% train knowAcc: 79.812% test knowAcc: 54.405%\n",
      "285\t train loss: 0.1045 train acc: 80.154% test acc: 51.394% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 78.702% test conAcc: 46.743% train knowAcc: 80.097% test knowAcc: 55.238%\n",
      "290\t train loss: 0.1041 train acc: 79.584% test acc: 51.926% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 78.556% test conAcc: 46.743% train knowAcc: 79.157% test knowAcc: 56.190%\n",
      "295\t train loss: 0.1022 train acc: 80.416% test acc: 51.461% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 79.030% test conAcc: 47.068% train knowAcc: 80.325% test knowAcc: 55.119%\n",
      "300\t train loss: 0.1018 train acc: 80.847% test acc: 50.863% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 80.124% test conAcc: 47.231% train knowAcc: 80.268% test knowAcc: 53.929%\n",
      "305\t train loss: 0.1006 train acc: 81.001% test acc: 51.527% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 79.905% test conAcc: 47.557% train knowAcc: 80.723% test knowAcc: 54.881%\n",
      "310\t train loss: 0.1003 train acc: 80.677% test acc: 51.594% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 80.452% test conAcc: 46.743% train knowAcc: 79.698% test knowAcc: 55.595%\n",
      "315\t train loss: 0.0990 train acc: 81.432% test acc: 51.527% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 80.598% test conAcc: 47.231% train knowAcc: 80.979% test knowAcc: 55.119%\n",
      "320\t train loss: 0.1007 train acc: 80.523% test acc: 51.527% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 78.483% test conAcc: 47.557% train knowAcc: 80.951% test knowAcc: 54.881%\n",
      "325\t train loss: 0.0977 train acc: 81.201% test acc: 51.328% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 79.796% test conAcc: 47.394% train knowAcc: 81.179% test knowAcc: 54.643%\n",
      "330\t train loss: 0.0975 train acc: 81.463% test acc: 51.660% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 80.708% test conAcc: 46.417% train knowAcc: 80.951% test knowAcc: 55.952%\n",
      "335\t train loss: 0.0966 train acc: 81.940% test acc: 51.129% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.255% test conAcc: 45.928% train knowAcc: 81.407% test knowAcc: 55.357%\n",
      "340\t train loss: 0.0971 train acc: 81.940% test acc: 50.863% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.072% test conAcc: 46.580% train knowAcc: 81.549% test knowAcc: 54.405%\n",
      "345\t train loss: 0.0954 train acc: 82.217% test acc: 51.129% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 80.999% test conAcc: 46.580% train knowAcc: 82.118% test knowAcc: 54.881%\n",
      "350\t train loss: 0.0951 train acc: 82.156% test acc: 51.195% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.838% test conAcc: 45.928% train knowAcc: 81.350% test knowAcc: 55.476%\n",
      "355\t train loss: 0.0944 train acc: 81.955% test acc: 51.992% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.473% test conAcc: 46.743% train knowAcc: 81.264% test knowAcc: 56.310%\n",
      "360\t train loss: 0.0946 train acc: 82.510% test acc: 50.863% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.291% test conAcc: 46.417% train knowAcc: 82.432% test knowAcc: 54.524%\n",
      "365\t train loss: 0.0927 train acc: 82.879% test acc: 51.328% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 82.093% test conAcc: 46.580% train knowAcc: 82.489% test knowAcc: 55.238%\n",
      "370\t train loss: 0.0937 train acc: 81.832% test acc: 52.125% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 81.437% test conAcc: 47.068% train knowAcc: 81.065% test knowAcc: 56.190%\n",
      "375\t train loss: 0.0923 train acc: 83.187% test acc: 50.664% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 82.385% test conAcc: 45.277% train knowAcc: 82.830% test knowAcc: 54.881%\n",
      "380\t train loss: 0.0940 train acc: 82.710% test acc: 50.930% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.473% test conAcc: 47.068% train knowAcc: 82.659% test knowAcc: 54.167%\n",
      "385\t train loss: 0.0914 train acc: 82.987% test acc: 50.465% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 81.911% test conAcc: 46.417% train knowAcc: 82.830% test knowAcc: 53.810%\n",
      "390\t train loss: 0.0908 train acc: 83.449% test acc: 50.398% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 82.495% test conAcc: 46.254% train knowAcc: 83.229% test knowAcc: 53.810%\n",
      "395\t train loss: 0.0905 train acc: 83.218% test acc: 51.195% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 82.130% test conAcc: 46.906% train knowAcc: 83.087% test knowAcc: 54.762%\n",
      "400\t train loss: 0.0896 train acc: 83.249% test acc: 51.262% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.115% test conAcc: 45.440% train knowAcc: 82.375% test knowAcc: 55.714%\n",
      "405\t train loss: 0.0897 train acc: 83.495% test acc: 50.863% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 82.166% test conAcc: 46.906% train knowAcc: 83.571% test knowAcc: 54.048%\n",
      "410\t train loss: 0.0888 train acc: 83.695% test acc: 49.934% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 82.896% test conAcc: 45.603% train knowAcc: 83.371% test knowAcc: 53.333%\n",
      "415\t train loss: 0.0887 train acc: 83.480% test acc: 50.996% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 82.349% test conAcc: 46.580% train knowAcc: 83.400% test knowAcc: 54.524%\n",
      "420\t train loss: 0.0889 train acc: 83.264% test acc: 51.859% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.406% test conAcc: 45.928% train knowAcc: 82.175% test knowAcc: 56.429%\n",
      "425\t train loss: 0.0880 train acc: 83.326% test acc: 51.394% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 81.802% test conAcc: 46.743% train knowAcc: 83.542% test knowAcc: 55.000%\n",
      "430\t train loss: 0.0871 train acc: 84.296% test acc: 50.598% train exerAcc: 97.510% test exerAcc: 44.231% train conAcc: 83.552% test conAcc: 46.743% train knowAcc: 83.969% test knowAcc: 53.810%\n",
      "435\t train loss: 0.0868 train acc: 84.219% test acc: 50.332% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 83.333% test conAcc: 45.928% train knowAcc: 83.998% test knowAcc: 53.810%\n",
      "440\t train loss: 0.0869 train acc: 83.926% test acc: 50.996% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.115% test conAcc: 46.906% train knowAcc: 83.628% test knowAcc: 54.167%\n",
      "445\t train loss: 0.0852 train acc: 84.465% test acc: 50.332% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 83.516% test conAcc: 45.928% train knowAcc: 84.311% test knowAcc: 53.810%\n",
      "450\t train loss: 0.0850 train acc: 84.650% test acc: 50.664% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 83.917% test conAcc: 46.091% train knowAcc: 84.339% test knowAcc: 54.286%\n",
      "455\t train loss: 0.0847 train acc: 84.527% test acc: 50.133% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 84.063% test conAcc: 46.254% train knowAcc: 83.998% test knowAcc: 53.214%\n",
      "460\t train loss: 0.0861 train acc: 84.496% test acc: 51.062% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.479% test conAcc: 46.417% train knowAcc: 84.396% test knowAcc: 54.643%\n",
      "465\t train loss: 0.0847 train acc: 84.788% test acc: 51.062% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 84.282% test conAcc: 46.906% train knowAcc: 84.311% test knowAcc: 54.286%\n",
      "470\t train loss: 0.0842 train acc: 84.727% test acc: 51.195% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.953% test conAcc: 47.231% train knowAcc: 84.453% test knowAcc: 54.286%\n",
      "475\t train loss: 0.0848 train acc: 84.557% test acc: 51.062% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.625% test conAcc: 47.068% train knowAcc: 84.396% test knowAcc: 54.167%\n",
      "480\t train loss: 0.0831 train acc: 84.496% test acc: 51.262% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.662% test conAcc: 47.068% train knowAcc: 84.254% test knowAcc: 54.524%\n",
      "485\t train loss: 0.0827 train acc: 85.173% test acc: 51.328% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 84.683% test conAcc: 47.231% train knowAcc: 84.710% test knowAcc: 54.524%\n",
      "490\t train loss: 0.0835 train acc: 84.942% test acc: 50.996% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 83.990% test conAcc: 47.720% train knowAcc: 84.823% test knowAcc: 53.690%\n",
      "495\t train loss: 0.0815 train acc: 85.389% test acc: 51.129% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 84.318% test conAcc: 47.720% train knowAcc: 85.393% test knowAcc: 53.929%\n",
      "500\t train loss: 0.0812 train acc: 85.327% test acc: 51.195% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 85.084% test conAcc: 46.906% train knowAcc: 84.681% test knowAcc: 54.524%\n",
      "505\t train loss: 0.0820 train acc: 85.173% test acc: 50.266% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 84.427% test conAcc: 46.417% train knowAcc: 84.909% test knowAcc: 53.214%\n",
      "510\t train loss: 0.0808 train acc: 85.481% test acc: 50.133% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 84.829% test conAcc: 45.440% train knowAcc: 85.165% test knowAcc: 53.810%\n",
      "515\t train loss: 0.0810 train acc: 85.358% test acc: 50.797% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 85.193% test conAcc: 45.928% train knowAcc: 84.653% test knowAcc: 54.643%\n",
      "520\t train loss: 0.0802 train acc: 85.142% test acc: 51.195% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 84.902% test conAcc: 46.254% train knowAcc: 84.482% test knowAcc: 55.000%\n",
      "525\t train loss: 0.0812 train acc: 85.235% test acc: 50.398% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 83.662% test conAcc: 46.254% train knowAcc: 85.621% test knowAcc: 53.571%\n",
      "530\t train loss: 0.0794 train acc: 85.774% test acc: 49.867% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 85.303% test conAcc: 46.254% train knowAcc: 85.336% test knowAcc: 52.738%\n",
      "535\t train loss: 0.0800 train acc: 85.650% test acc: 50.531% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 84.756% test conAcc: 46.906% train knowAcc: 85.535% test knowAcc: 53.452%\n",
      "540\t train loss: 0.0794 train acc: 85.358% test acc: 51.328% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 84.391% test conAcc: 47.231% train knowAcc: 85.279% test knowAcc: 54.524%\n",
      "545\t train loss: 0.0792 train acc: 85.820% test acc: 49.867% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 85.376% test conAcc: 45.928% train knowAcc: 85.364% test knowAcc: 52.857%\n",
      "550\t train loss: 0.0782 train acc: 85.928% test acc: 50.133% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 85.120% test conAcc: 45.603% train knowAcc: 85.763% test knowAcc: 53.690%\n",
      "555\t train loss: 0.0782 train acc: 85.881% test acc: 50.863% train exerAcc: 97.510% test exerAcc: 46.154% train conAcc: 85.777% test conAcc: 45.765% train knowAcc: 85.165% test knowAcc: 54.881%\n",
      "560\t train loss: 0.0777 train acc: 85.650% test acc: 50.930% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 85.120% test conAcc: 46.417% train knowAcc: 85.251% test knowAcc: 54.405%\n",
      "565\t train loss: 0.0779 train acc: 86.313% test acc: 50.066% train exerAcc: 97.510% test exerAcc: 48.077% train conAcc: 85.777% test conAcc: 45.603% train knowAcc: 85.962% test knowAcc: 53.452%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21300\\1319696952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;31m# torch.autograd.set_detect_anomaly(True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mrep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mrep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"_kw\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;31m# results = self.evaluate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainPhysical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainPhysical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m             \u001b[0mlh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;31m# update our predictions as features when appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, opts)\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss1\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mloss1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainConsumption\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainKnowledge\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainPhysical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[1;31m# print(\"????\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\mdiabetes\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\mdiabetes\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model, learning_rate = \"BasicNNSplit\", .006\n",
    "model, learning_rate = \"BasicNN\", .0054\n",
    "# model, learning_rate = \"LogisticRegressor\", .003\n",
    "# model, learning_rate = \"AdaptableLSTM\", .1\n",
    "epochs = 1000\n",
    "seed = 2\n",
    "include_state = True\n",
    "estate = True\n",
    "fullq = False\n",
    "respond_perc = .50\n",
    "# number of weeks history to include as features\n",
    "numWeeks = 2\n",
    "insertpreds = True\n",
    "noise = 0.00\n",
    "smooth = 0\n",
    "# one question = one row if true\n",
    "splitQs = False\n",
    "# uses separate layers (or whole models if not LSTM) for question categories\n",
    "splitModel = True\n",
    "\n",
    "# format for these is [offEpoch, onEpoch, offEpoch, onEpoch, offEpoch....]\n",
    "# toggles training the category at every epoch in the list\n",
    "# if any category is disabled, LSTM block will be frozen\n",
    "knowSched = [1000]\n",
    "physSched = [1000]\n",
    "conSched = [1000]\n",
    "\n",
    "    \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "e = Experiment(\n",
    "    modelSplit = splitModel,\n",
    "    numValFolds = 5,\n",
    "    epochsToUpdateLabelMods = 10,\n",
    "    knowSchedule = knowSched,\n",
    "    consumpSchedule = conSched,\n",
    "    physSchedule = physSched,\n",
    "    data_kw={\"minw\": 2,\n",
    "            \"maxw\": 31,\n",
    "            \"include_state\": include_state,\n",
    "            \"include_pid\": False,\n",
    "            \"expanded_states\": estate,\n",
    "            \"top_respond_perc\": respond_perc,\n",
    "             \"full_questionnaire\": fullq,\n",
    "             \"num_weeks_history\": numWeeks,\n",
    "             \"insert_predictions\": insertpreds,\n",
    "             \"one_hot_response_features\": True,\n",
    "             \"response_feature_noise\": noise,\n",
    "             \"max_state_week\": 30,\n",
    "             \"split_model_features\": splitModel,\n",
    "             \"split_weekly_questions\": splitQs\n",
    "            },\n",
    "    model=model,\n",
    "    model_kw={\n",
    "        \"lossfn\": \"MSELoss\",\n",
    "        # \"lossfn\": \"NDCG\",\n",
    "        # \"lossfn\": \"CrossEntropyLoss\",\n",
    "        \"hidden_size\": 50,\n",
    "        \"lr_step_mult\": 1.0, \n",
    "        \"lr_step_epochs\": 100,\n",
    "        \"opt_kw\": {\n",
    "            \"lr\": learning_rate,\n",
    "            \"weight_decay\": 0\n",
    "        },\n",
    "        \"splitModel\": splitModel,\n",
    "        \"splitWeeklyQuestions\": splitQs,\n",
    "        \"labelSmoothPerc\": smooth,\n",
    "        \"gaussianNoiseStd\": noise\n",
    "        \n",
    "    },\n",
    "    train_kw={\n",
    "        \"epochs\": epochs,\n",
    "        \"n_subj\": 500,\n",
    "        \"rec_every\": 5,\n",
    "    })\n",
    "\n",
    "\n",
    "print(len(e.bd.test))\n",
    "print(len(e.bd.train))\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "report = e.run()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de6743-690e-463b-b5b0-212eb0da9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (np.mean(report['train_metrics'], axis=0))\n",
    "labels = report[\"metric_labels\"]\n",
    "print(report['train_metrics'][-1, labels.index(\"Acc\")])\n",
    "print(report['test_metrics'][-1, labels.index(\"Acc\")])\n",
    "\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"Acc\")], label=\"Train Acc.\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"Acc\")], label=\"Test Acc.\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"MSE\")], label=\"Train MSE\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"MSE\")], label=\"Test MSE\")\n",
    "plt.title(\"Train/Test Performance Over Training\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.savefig(\"simpleNotebookAccPlot.png\")\n",
    "\n",
    "plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5928e0-08ac-4055-9bc6-89fe1fea6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e.bd.features.shape, e.bd.featureList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41511c-c598-4167-9570-c6b6a75c927a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6e339f590beffb2e62e02c6be9b431caf4c76db3ef9baeb9786d6033ee27a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
