{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23791e19-fa29-4321-9a92-62c66249e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "from utils.behavior_data import BehaviorData\n",
    "from visuals import Plotter\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.state_data import StateData\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ccab30-3fb9-4990-8492-e7d81a539a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 537\n",
      "paction_sids\n",
      "pmsg_ids\n",
      "qids\n",
      "response\n",
      "(12912, 41) (12912, 4) 41\n",
      "['state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state' 'state'\n",
      " 'state' 'state' 'state' 'state' 'paction_sids_last_0_q1'\n",
      " 'paction_sids_last_0_q1' 'paction_sids_last_0_q1'\n",
      " 'paction_sids_last_0_q1' 'paction_sids_last_0_q1' 'pmsg_ids_last_0_q1'\n",
      " 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q1'\n",
      " 'pmsg_ids_last_0_q1' 'pmsg_ids_last_0_q1' 'qids_last_0_q1'\n",
      " 'qids_last_0_q1' 'qids_last_0_q1' 'qids_last_0_q1' 'qids_last_0_q1'\n",
      " 'qids_last_0_q1' 'q1_cat' 'q1_cat']\n",
      "54\n",
      "214\n",
      "0\t train loss: 0.1988 train acc: 31.840% test acc: 33.798% train exerAcc: 34.440% test exerAcc: 36.538%\n",
      "5\t train loss: 0.1837 train acc: 47.360% test acc: 44.223% train exerAcc: 48.548% test exerAcc: 50.000%\n",
      "10\t train loss: 0.1696 train acc: 56.059% test acc: 51.594% train exerAcc: 48.133% test exerAcc: 50.000%\n",
      "15\t train loss: 0.1614 train acc: 57.521% test acc: 54.515% train exerAcc: 48.963% test exerAcc: 50.000%\n",
      "20\t train loss: 0.1602 train acc: 58.260% test acc: 55.312% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "25\t train loss: 0.1568 train acc: 58.707% test acc: 54.781% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "30\t train loss: 0.1720 train acc: 58.676% test acc: 55.710% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "35\t train loss: 0.1709 train acc: 58.968% test acc: 55.511% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "40\t train loss: 0.1702 train acc: 58.830% test acc: 55.445% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "45\t train loss: 0.1677 train acc: 58.707% test acc: 55.644% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "50\t train loss: 0.1664 train acc: 58.707% test acc: 55.710% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "55\t train loss: 0.1655 train acc: 58.722% test acc: 55.843% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "60\t train loss: 0.1646 train acc: 59.169% test acc: 56.441% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "65\t train loss: 0.1638 train acc: 59.523% test acc: 56.308% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "70\t train loss: 0.1618 train acc: 59.523% test acc: 55.910% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "75\t train loss: 0.1609 train acc: 59.846% test acc: 56.308% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "80\t train loss: 0.1600 train acc: 59.908% test acc: 55.976% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "85\t train loss: 0.1596 train acc: 59.938% test acc: 55.976% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "90\t train loss: 0.1592 train acc: 60.154% test acc: 55.976% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "95\t train loss: 0.1578 train acc: 60.308% test acc: 55.910% train exerAcc: 49.378% test exerAcc: 50.000%\n",
      "100\t train loss: 0.1732 train acc: 60.662% test acc: 55.843% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "105\t train loss: 0.1721 train acc: 60.647% test acc: 55.777% train exerAcc: 50.622% test exerAcc: 51.923%\n",
      "110\t train loss: 0.1690 train acc: 60.585% test acc: 55.777% train exerAcc: 50.207% test exerAcc: 50.000%\n",
      "115\t train loss: 0.1704 train acc: 60.585% test acc: 56.042% train exerAcc: 51.452% test exerAcc: 53.846%\n",
      "120\t train loss: 0.1706 train acc: 60.539% test acc: 55.976% train exerAcc: 51.452% test exerAcc: 53.846%\n",
      "125\t train loss: 0.1693 train acc: 60.585% test acc: 55.976% train exerAcc: 53.112% test exerAcc: 55.769%\n",
      "130\t train loss: 0.1665 train acc: 60.539% test acc: 55.843% train exerAcc: 52.282% test exerAcc: 51.923%\n",
      "135\t train loss: 0.1639 train acc: 60.570% test acc: 55.976% train exerAcc: 53.112% test exerAcc: 55.769%\n",
      "140\t train loss: 0.1641 train acc: 60.631% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 55.769%\n",
      "145\t train loss: 0.1633 train acc: 60.631% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 55.769%\n",
      "150\t train loss: 0.1635 train acc: 60.631% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 55.769%\n",
      "155\t train loss: 0.1649 train acc: 60.631% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 55.769%\n",
      "160\t train loss: 0.1629 train acc: 60.600% test acc: 55.843% train exerAcc: 53.942% test exerAcc: 51.923%\n",
      "165\t train loss: 0.1617 train acc: 60.585% test acc: 55.843% train exerAcc: 53.527% test exerAcc: 51.923%\n",
      "170\t train loss: 0.1618 train acc: 60.616% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 55.769%\n",
      "175\t train loss: 0.1641 train acc: 60.616% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 55.769%\n",
      "180\t train loss: 0.1432 train acc: 61.047% test acc: 56.242% train exerAcc: 54.357% test exerAcc: 55.769%\n",
      "185\t train loss: 0.1353 train acc: 61.401% test acc: 56.109% train exerAcc: 54.357% test exerAcc: 55.769%\n",
      "190\t train loss: 0.1349 train acc: 61.401% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "195\t train loss: 0.1324 train acc: 61.401% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "200\t train loss: 0.1319 train acc: 61.401% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "205\t train loss: 0.1312 train acc: 61.401% test acc: 55.976% train exerAcc: 54.357% test exerAcc: 51.923%\n",
      "210\t train loss: 0.1310 train acc: 61.432% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "215\t train loss: 0.1299 train acc: 61.432% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "220\t train loss: 0.1296 train acc: 61.416% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "225\t train loss: 0.1296 train acc: 61.416% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "230\t train loss: 0.1296 train acc: 61.416% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "235\t train loss: 0.1283 train acc: 61.432% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "240\t train loss: 0.1286 train acc: 61.463% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "245\t train loss: 0.1293 train acc: 61.478% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "250\t train loss: 0.1281 train acc: 61.493% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "255\t train loss: 0.1282 train acc: 61.493% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "260\t train loss: 0.1283 train acc: 61.493% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "265\t train loss: 0.1291 train acc: 61.509% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "270\t train loss: 0.1283 train acc: 61.509% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "275\t train loss: 0.1283 train acc: 61.493% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "280\t train loss: 0.1278 train acc: 61.493% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "285\t train loss: 0.1281 train acc: 61.509% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "290\t train loss: 0.1278 train acc: 61.509% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "295\t train loss: 0.1281 train acc: 61.509% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "300\t train loss: 0.1284 train acc: 61.524% test acc: 56.042% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "305\t train loss: 0.1283 train acc: 61.540% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "310\t train loss: 0.1281 train acc: 61.540% test acc: 56.042% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "315\t train loss: 0.1283 train acc: 61.555% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "320\t train loss: 0.1278 train acc: 61.570% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "325\t train loss: 0.1268 train acc: 61.555% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "330\t train loss: 0.1277 train acc: 61.570% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "335\t train loss: 0.1277 train acc: 61.555% test acc: 55.976% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "340\t train loss: 0.1276 train acc: 61.570% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "345\t train loss: 0.1281 train acc: 61.570% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "350\t train loss: 0.1276 train acc: 61.570% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "355\t train loss: 0.1271 train acc: 61.570% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "360\t train loss: 0.1278 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "365\t train loss: 0.1287 train acc: 61.570% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "370\t train loss: 0.1274 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "375\t train loss: 0.1283 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "380\t train loss: 0.1279 train acc: 61.570% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "385\t train loss: 0.1273 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "390\t train loss: 0.1275 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "395\t train loss: 0.1267 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "400\t train loss: 0.1277 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "405\t train loss: 0.1276 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "410\t train loss: 0.1280 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "415\t train loss: 0.1274 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "420\t train loss: 0.1278 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "425\t train loss: 0.1282 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "430\t train loss: 0.1281 train acc: 61.586% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "435\t train loss: 0.1278 train acc: 61.601% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "440\t train loss: 0.1267 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "445\t train loss: 0.1277 train acc: 61.601% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "450\t train loss: 0.1285 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "455\t train loss: 0.1278 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "460\t train loss: 0.1273 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "465\t train loss: 0.1280 train acc: 61.617% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "470\t train loss: 0.1274 train acc: 61.601% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "475\t train loss: 0.1272 train acc: 61.617% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "480\t train loss: 0.1271 train acc: 61.632% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "485\t train loss: 0.1275 train acc: 61.601% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "490\t train loss: 0.1273 train acc: 61.647% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "495\t train loss: 0.1281 train acc: 61.632% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "500\t train loss: 0.1281 train acc: 61.632% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "505\t train loss: 0.1272 train acc: 61.632% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "510\t train loss: 0.1282 train acc: 61.632% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "515\t train loss: 0.1274 train acc: 61.647% test acc: 55.843% train exerAcc: 54.772% test exerAcc: 50.000%\n",
      "520\t train loss: 0.1267 train acc: 61.632% test acc: 55.910% train exerAcc: 54.772% test exerAcc: 50.000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34108\\3628252348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;31m# torch.autograd.set_detect_anomaly(True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mrep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mrep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"_kw\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;31m# results = self.evaluate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[1;31m#     print(param)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0mstored_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport_scores_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m                 \u001b[0mtrain_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[0mtmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\experiment.py\u001b[0m in \u001b[0;36mreport_scores_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    532\u001b[0m                     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m             \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport_scores_min\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\models\\base.py\u001b[0m in \u001b[0;36mreport_scores_min\u001b[1;34m(self, y, pred, data)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mceloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mcrit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNDCG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mndcg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mcrit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMRR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mmrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Code\\HealthLearning\\mdiabetes-behavior-modeling\\models\\ModelUtils.py\u001b[0m in \u001b[0;36mNDCG\u001b[1;34m(pred, y)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mranks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# best rank should be 1 not 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdcg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mranks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model, learning_rate = \"BasicNNSplit\", .006\n",
    "model, learning_rate = \"BasicNN\", .0054\n",
    "# model, learning_rate = \"LogisticRegressor\", .003\n",
    "# model, learning_rate = \"AdaptableLSTM\", .1\n",
    "epochs = 1000\n",
    "seed = 2\n",
    "include_state = True\n",
    "estate = True\n",
    "fullq = False\n",
    "respond_perc = .50\n",
    "numWeeks = 1\n",
    "insertpreds = True\n",
    "noise = 0.07\n",
    "smooth = 0\n",
    "splitQs = True\n",
    "splitModel = True\n",
    "\n",
    "# format for these is [offEpoch, onEpoch, offEpoch, onEpoch, offEpoch....]\n",
    "# toggles training the category at every epoch in the list\n",
    "# if any category is disabled, LSTM block will be frozen\n",
    "knowSched = [30, 180]\n",
    "physSched = [30, 100, 180]\n",
    "conSched = [100]\n",
    "\n",
    "    \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "e = Experiment(\n",
    "    modelSplit = splitModel,\n",
    "    numValFolds = 5,\n",
    "    epochsToUpdateLabelMods = 10,\n",
    "    knowSchedule = knowSched,\n",
    "    consumpSchedule = conSched,\n",
    "    physSchedule = physSched,\n",
    "    data_kw={\"minw\": 2,\n",
    "            \"maxw\": 31,\n",
    "            \"include_state\": include_state,\n",
    "            \"include_pid\": False,\n",
    "            \"expanded_states\": estate,\n",
    "            \"top_respond_perc\": respond_perc,\n",
    "             \"full_questionnaire\": fullq,\n",
    "             \"num_weeks_history\": numWeeks,\n",
    "             \"insert_predictions\": insertpreds,\n",
    "             \"one_hot_response_features\": True,\n",
    "             \"response_feature_noise\": noise,\n",
    "             \"max_state_week\": 1,\n",
    "             \"split_model_features\": splitModel,\n",
    "             \"split_weekly_questions\": splitQs\n",
    "            },\n",
    "    model=model,\n",
    "    model_kw={\n",
    "        \"lossfn\": \"MSELoss\",\n",
    "        # \"lossfn\": \"NDCG\",\n",
    "        # \"lossfn\": \"CrossEntropyLoss\",\n",
    "        \"hidden_size\": 10,\n",
    "        \"lr_step_mult\": .9, \n",
    "        \"lr_step_epochs\": 100,\n",
    "        \"opt_kw\": {\n",
    "            \"lr\": learning_rate,\n",
    "        },\n",
    "        \"splitModel\": splitModel,\n",
    "        \"splitWeeklyQuestions\": splitQs,\n",
    "        \"labelSmoothPerc\": smooth,\n",
    "        \"gaussianNoiseStd\": noise\n",
    "        \n",
    "    },\n",
    "    train_kw={\n",
    "        \"epochs\": epochs,\n",
    "        \"n_subj\": 500,\n",
    "        \"rec_every\": 5,\n",
    "    })\n",
    "\n",
    "\n",
    "print(len(e.bd.test))\n",
    "print(len(e.bd.train))\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "report = e.run()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de6743-690e-463b-b5b0-212eb0da9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (np.mean(report['train_metrics'], axis=0))\n",
    "labels = report[\"metric_labels\"]\n",
    "print(report['train_metrics'][-1, labels.index(\"Acc\")])\n",
    "print(report['test_metrics'][-1, labels.index(\"Acc\")])\n",
    "\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"Acc\")], label=\"Train Acc.\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"Acc\")], label=\"Test Acc.\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"train_metrics\"][:, labels.index(\"MSE\")], label=\"Train MSE\")\n",
    "splot = plt.plot(report[\"rec_epochs\"], report[\"test_metrics\"][:, labels.index(\"MSE\")], label=\"Test MSE\")\n",
    "plt.title(\"Train/Test Performance Over Training\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.savefig(\"simpleNotebookAccPlot.png\")\n",
    "\n",
    "plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5928e0-08ac-4055-9bc6-89fe1fea6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e.bd.features.shape, e.bd.featureList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41511c-c598-4167-9570-c6b6a75c927a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6e339f590beffb2e62e02c6be9b431caf4c76db3ef9baeb9786d6033ee27a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
